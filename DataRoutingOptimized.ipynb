{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import rtree\n",
    "from rtree import index\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk_row(row, used_dims, pidx, pid_data_dict):\n",
    "    row_numpy = row.to_numpy()\n",
    "    row_used_dims_list = row_numpy[used_dims].tolist()\n",
    "    row_border = tuple(row_used_dims_list+row_used_dims_list)\n",
    "    pid = list(pidx.intersection(row_border))[0]\n",
    "    pid_data_dict[pid].append(row_numpy.tolist())\n",
    "\n",
    "\n",
    "class DRThread(threading.Thread):\n",
    "    def __init__(self, thread_id, name, parameters):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.thread_id = thread_id\n",
    "        self.name = name\n",
    "        self.parameters = parameters\n",
    "        \n",
    "    def run(self):\n",
    "        print('start thread: ', self.thread_id, self.name)\n",
    "        chunk, used_dims, pidx, pid_data_dict = self.parameters\n",
    "        chunk.apply(lambda row: process_chunk_row(row, used_dims, pidx, pid_data_dict), axis=1)\n",
    "        print('exit thread: ', self.thread_id, self.name)\n",
    "        \n",
    "class DumpThread(threading.Thread):\n",
    "    def __init__(self, thread_id, name, parameters):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.thread_id = thread_id\n",
    "        self.name = name\n",
    "        self.parameters = parameters\n",
    "        \n",
    "    def run(self):\n",
    "        print('start dumping thread: ', self.thread_id, self.name)\n",
    "        start_index, end_index, pids, pid_data_dict, hdfs_path, column_names = self.parameters\n",
    "        for pid in pids[start_index: end_index]:\n",
    "            path = hdfs_path + 'partition_' + str(pid)+'.parquet'\n",
    "            pdf = pd.DataFrame(pid_data_dict[pid], columns=column_names)\n",
    "            df = sqlContext.createDataFrame(pdf)\n",
    "            df.write.mode('append').parquet(path)\n",
    "        print('exit dumping thread: ', self.thread_id, self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kdnode_2_border(kdnode):\n",
    "    lower = [domain[0] for domain in kdnode[0]]\n",
    "    upper = [domain[1] for domain in kdnode[0]]\n",
    "    border = tuple(lower + upper) # non interleave\n",
    "    return border\n",
    "\n",
    "def load_partitions_from_file(path):\n",
    "    '''\n",
    "    the loaded stretched_kdnodes: [num_dims, l1,l2,...,ln, u1,u2,...,un, size, id, pid, left_child,id, right_child_id]\n",
    "    '''\n",
    "    stretched_kdnodes = genfromtxt(path, delimiter=',')\n",
    "    num_dims = int(stretched_kdnodes[0,0])\n",
    "    kdnodes = []\n",
    "    for i in range(len(stretched_kdnodes)):\n",
    "        domains = [ [stretched_kdnodes[i,k+1],stretched_kdnodes[i,1+num_dims+k]] for k in range(num_dims) ]\n",
    "        row = [domains]\n",
    "        row.append(stretched_kdnodes[i,2*num_dims+1])\n",
    "        # to be compatible with qd-tree's partition, that do not have the last 4 attributes\n",
    "        if len(stretched_kdnodes[i]) > 2*num_dims+2:\n",
    "            row.append(stretched_kdnodes[i,-4])\n",
    "            row.append(stretched_kdnodes[i,-3])\n",
    "            row.append(stretched_kdnodes[i,-2])\n",
    "            row.append(stretched_kdnodes[i,-1])\n",
    "        kdnodes.append(row)\n",
    "    return kdnodes\n",
    "\n",
    "# def dump_data_thread(start_index, end_index, pids, pid_data_dict, hdfs_path):\n",
    "#     for pid in pids[start_index, end_index]:\n",
    "#         path = hdfs_path + 'partition_' + str(pid)+'.parquet'\n",
    "#         pdf = pd.DataFrame(pid_data_dict[pid], columns=column_names)\n",
    "#         df = sqlContext.createDataFrame(pdf)\n",
    "#         df.write.mode('append').parquet(path)\n",
    "\n",
    "def dump_dict_data_2_hdfs(pid_data_dict, column_names, hdfs_path, num_threads = 8):\n",
    "    pids = list(pid_data_dict.keys())\n",
    "    step = int(len(pids) / num_threads) + 1\n",
    "    threads = []\n",
    "    for i in range(num_threads):\n",
    "        start_index = i * step\n",
    "        end_index = (i+1) * step\n",
    "        parameters = [start_index, end_index, pids, pid_data_dict, hdfs_path, column_names]\n",
    "        thread = DumpThread(i, 'dump_thread_'+str(i), parameters)\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "        if start_index >= len(pids):\n",
    "            break   \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "\n",
    "def batch_data_parallel(table_path, partition_path, chunk_size, used_dims, hdfs_path, \n",
    "                        num_dims, dump_threshold = 1000000, max_threads = 8):\n",
    "    \n",
    "    begin_time = time.time()\n",
    "    \n",
    "    col_names = ['_c'+str(i) for i in range(num_dims)]\n",
    "    cols = [i for i in range(num_dims)]\n",
    "    \n",
    "    partitions = load_partitions_from_file(partition_path)\n",
    "    \n",
    "    p = index.Property()\n",
    "    p.leaf_capacity = 32\n",
    "    p.index_capacity =32\n",
    "    p.NearMinimumOverlaoFactor = 16\n",
    "    p.fill_factor = 0.8\n",
    "    p.overwrite = True\n",
    "    \n",
    "    pidxs = [] # the rtree index has problem in mutli-threading, create an index for each thread\n",
    "    for k in range(num_threads):\n",
    "        partition_index = index.Index(properties = p)\n",
    "        for i in range(len(partitions)):\n",
    "            #partition_index.insert(int(partitions[i][-4]), kdnode_2_border(partitions[i])) \n",
    "            partition_index.insert(i, kdnode_2_border(partitions[i]))\n",
    "        pidxs.append(partition_index)\n",
    "    \n",
    "    pid_data_dict = {}\n",
    "    for i in range(len(partitions)):\n",
    "        pid_data_dict.update({i:[]})\n",
    "    \n",
    "    count = 0\n",
    "    epochs = 0\n",
    "    processed_data = 0\n",
    "    threads = []\n",
    "    #for chunk in pd.read_table(table_path, delimiter='|', usecols=cols, names=col_names, chunksize=chunk_size):\n",
    "    for chunk in pd.read_csv(table_path, usecols=cols, names=col_names, chunksize=chunk_size):\n",
    "        \n",
    "        print('current chunk: ', count)\n",
    "        tid = count % max_threads      \n",
    "        parameters = [chunk, used_dims, pidxs[tid], pid_data_dict]\n",
    "        thread = DRThread(tid, 'thread_'+str(tid)+'_'+str(count), parameters)\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "        count += 1\n",
    "        \n",
    "        if tid == max_threads-1:\n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            threads = []\n",
    "            epochs += 1\n",
    "            processed_data += chunk_size * max_threads\n",
    "            if processed_data >= dump_threshold:\n",
    "                dump_dict_data_2_hdfs(pid_data_dict, col_names, hdfs_path)\n",
    "                for key in pid_data_dict.keys():\n",
    "                    pid_data_dict[key]=[]\n",
    "                processed_data = 0\n",
    "                \n",
    "            print('===================================================')\n",
    "    dump_dict_data_2_hdfs(pid_data_dict, col_names, hdfs_path) # last batch\n",
    "    \n",
    "    finish_time = time.time()\n",
    "    print('total data routing and persisting time: ', finish_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current chunk:  0\n",
      "start thread:  0 thread_0_0\n",
      "current chunk:  1\n",
      "start thread:  1 thread_1_1\n",
      "current chunk:  2\n",
      "start thread:  2 thread_2_2\n",
      "current chunk:  3\n",
      "start thread:  3 thread_3_3\n",
      "current chunk:  4\n",
      "start thread:  4 thread_4_4\n",
      "current chunk:  5\n",
      "start thread:  5 thread_5_5\n",
      "current chunk:  6\n",
      "start thread:  6 thread_6_6\n",
      "current chunk:  7\n",
      "start thread:  7 thread_7_7\n",
      "exit thread:  1 thread_1_1\n",
      "exit thread:  3 thread_3_3\n",
      "exit thread:  4 thread_4_4\n",
      "exit thread:  0 thread_0_0\n",
      "exit thread:  7 thread_7_7\n",
      "exit thread:  6 thread_6_6\n",
      "exit thread:  5 thread_5_5\n",
      "exit thread:  2 thread_2_2\n",
      "===================================================\n",
      "current chunk:  8\n",
      "start thread:  0 thread_0_8\n",
      "current chunk:  9\n",
      "start thread:  1 thread_1_9\n",
      "current chunk:  10\n",
      "start thread:  2 thread_2_10\n",
      "current chunk:  11\n",
      "start thread:  3 thread_3_11\n",
      "current chunk:  12\n",
      "start thread:  4 thread_4_12\n",
      "current chunk:  13\n",
      "start thread:  5 thread_5_13\n",
      "current chunk:  14\n",
      "start thread:  6 thread_6_14\n",
      "current chunk:  15\n",
      "start thread:  7 thread_7_15\n",
      "exit thread:  6 thread_6_14\n",
      "exit thread:  4 thread_4_12\n",
      "exit thread:  2 thread_2_10\n",
      "exit thread:  0 thread_0_8\n",
      "exit thread:  7 thread_7_15\n",
      "exit thread:  3 thread_3_11\n",
      "exit thread:  1 thread_1_9\n",
      "exit thread:  5 thread_5_13\n",
      "start dumping thread:  0 dump_thread_0\n",
      "start dumping thread:  1 dump_thread_1\n",
      "start dumping thread:  2 dump_thread_2\n",
      "start dumping thread:  3 dump_thread_3\n",
      "start dumping thread:  4 dump_thread_4\n",
      "start dumping thread:  5 dump_thread_5\n",
      "start dumping thread:  6 dump_thread_6\n",
      "start dumping thread:  7 dump_thread_7\n",
      "exit dumping thread:  7 dump_thread_7\n",
      "exit dumping thread:  6 dump_thread_6\n",
      "exit dumping thread:  2 dump_thread_2\n",
      "exit dumping thread:  4 dump_thread_4\n",
      "exit dumping thread:  5 dump_thread_5\n",
      "exit dumping thread:  0 dump_thread_0\n",
      "exit dumping thread:  3 dump_thread_3\n",
      "exit dumping thread:  1 dump_thread_1\n",
      "===================================================\n",
      "current chunk:  16\n",
      "start thread:  0 thread_0_16\n",
      "current chunk:  17\n",
      "start thread:  1 thread_1_17\n",
      "current chunk:  18\n",
      "start thread:  2 thread_2_18\n",
      "current chunk:  19\n",
      "start thread:  3 thread_3_19\n",
      "current chunk:  20\n",
      "start thread:  4 thread_4_20\n",
      "current chunk:  21\n",
      "start thread:  5 thread_5_21\n",
      "current chunk:  22\n",
      "start thread:  6 thread_6_22\n",
      "current chunk:  23\n",
      "start thread:  7 thread_7_23\n",
      "exit thread:  1 thread_1_17\n",
      "exit thread:  2 thread_2_18\n",
      "exit thread:  3 thread_3_19\n",
      "exit thread:  6 thread_6_22\n",
      "exit thread:  0 thread_0_16\n",
      "exit thread:  5 thread_5_21\n",
      "exit thread:  7 thread_7_23\n",
      "exit thread:  4 thread_4_20\n",
      "===================================================\n",
      "current chunk:  24\n",
      "start thread:  0 thread_0_24\n",
      "current chunk:  25\n",
      "start thread:  1 thread_1_25\n",
      "current chunk:  26\n",
      "start thread:  2 thread_2_26\n",
      "current chunk:  27\n",
      "start thread:  3 thread_3_27\n",
      "current chunk:  28\n",
      "start thread:  4 thread_4_28\n",
      "current chunk:  29\n",
      "start thread:  5 thread_5_29\n",
      "current chunk:  30\n",
      "start thread:  6 thread_6_30\n",
      "current chunk:  31\n",
      "start thread:  7 thread_7_31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ab6976e68d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m batch_data_parallel(table_path, partition_path, chunk_size, used_dims, hdfs_path,\n\u001b[0;32m---> 12\u001b[0;31m                     num_dims, dump_threshold = 100000, max_threads = 8)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-cd368af6de75>\u001b[0m in \u001b[0;36mbatch_data_parallel\u001b[0;34m(table_path, partition_path, chunk_size, used_dims, hdfs_path, num_dims, dump_threshold, max_threads)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_threads\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mthreads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mepochs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# = = = Unit Test = = =\n",
    "table_path = '/home/cloudray/Downloads/TPCH_12M_8Field.csv'\n",
    "# table_path = '/home/cloudray/TPCH/2.18.0_rc2/dbgen/lineitem.tbl'\n",
    "partition_path = '/home/cloudray/NORA_Partitions/qd_tree_partitions'\n",
    "num_threads = 8\n",
    "num_dims = 8\n",
    "chunk_size = 10000\n",
    "used_dims = [1,2]\n",
    "hdfs_path = 'hdfs://localhost:9000/user/cloudray/QdTree/'\n",
    "\n",
    "batch_data_parallel(table_path, partition_path, chunk_size, used_dims, hdfs_path,\n",
    "                    num_dims, dump_threshold = 1000000, max_threads = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try concurrent write to hdfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
