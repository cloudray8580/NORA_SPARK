{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.master('local').appName('myAppName').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(SparkFiles.get('/home/cloudray/Downloads/TPCH_12M_8Field.csv'), header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_rows = df.filter(df._c2 < 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_rdd = sc.parallelize(subset_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = subset_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n",
    "subset_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to parquet (HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('hdfs://localhost:9000/user/cloudray/NORA/partition_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.write.parquet('hdfs://localhost:9000/user/cloudray/NORA/partition_1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from parquet (HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = sqlContext.read.parquet('hdfs://localhost:9000/user/cloudray/NORA/partition_1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_c0: double, _c1: double, _c2: double, _c3: double, _c4: double, _c5: double, _c6: double, _c7: double]\n"
     ]
    }
   ],
   "source": [
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0=21378.0, _c1=400000.0, _c2=1.0, _c3=3.0, _c4=47.0, _c5=51699.53, _c6=0.01, _c7=0.06)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.filter(df2._c2 < 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing locading multiple parquet files at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_file_paths(partition_ids):\n",
    "    \n",
    "    hdfs_path = 'hdfs://localhost:9000/user/cloudray/NORA/'\n",
    "    result_paths = []\n",
    "    \n",
    "    for pid in partition_ids:\n",
    "        partition_name = 'partition_' + str(pid)+'.parquet'\n",
    "        path = hdfs_path + partition_name\n",
    "        result_paths.append(path)\n",
    "        \n",
    "    return result_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_parquet_file_paths([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs://localhost:9000/user/cloudray/NORA/partition_0.parquet', 'hdfs://localhost:9000/user/cloudray/NORA/partition_1.parquet']\n"
     ]
    }
   ],
   "source": [
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = sqlContext.read.parquet(*paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_c0: double, _c1: double, _c2: double, _c3: double, _c4: double, _c5: double, _c6: double, _c7: double]\n"
     ]
    }
   ],
   "source": [
    "print(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11998567"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_pass_sqlcontext(context, df):\n",
    "    df = context.read.parquet('hdfs://localhost:9000/user/cloudray/NORA/partition_1.parquet')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_pass_sqlcontext(sqlContext, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11998567"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multi Thread Parquet File Writing\n",
    "import threading\n",
    "class myThread(threading.Thread):\n",
    "    def __init__(self, thread_id, name, df, lock_dict):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.thread_id = thread_id\n",
    "        self.name = name\n",
    "        self.df = df\n",
    "        self.lock_dict = lock_dict\n",
    "        \n",
    "    def run(self):\n",
    "        print('start thread: ',self.thread_id, self.name)\n",
    "        pid = 0\n",
    "        self.lock_dict[pid].acquire()\n",
    "        self.df.write.mode('append').parquet('hdfs://localhost:9000/user/cloudray/NORA/partition_TEST.parquet')\n",
    "        self.lock_dict[pid].release()\n",
    "\n",
    "max_threads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pdf1 = pd.DataFrame(np.array([[1,2],[3,4]]))\n",
    "df1 = sqlContext.createDataFrame(pdf1)\n",
    "\n",
    "pdf2 = pd.DataFrame(np.array([[11,12],[13,14]]))\n",
    "df2 = sqlContext.createDataFrame(pdf2)\n",
    "\n",
    "pdf3 = pd.DataFrame(np.array([[21,22],[23,24]]))\n",
    "df3 = sqlContext.createDataFrame(pdf3)\n",
    "\n",
    "pdf4 = pd.DataFrame(np.array([[31,32],[33,34]]))\n",
    "df4 = sqlContext.createDataFrame(pdf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to use lock dict\n",
    "lock_dict = {0:threading.Lock()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start thread:  1 thread_1\n",
      "start thread:  2 thread_2\n",
      "start thread:  3 thread_3\n",
      "start thread:  4 thread_4\n"
     ]
    }
   ],
   "source": [
    "thread1 = myThread(1, 'thread_'+str(1), df1, lock_dict)\n",
    "thread2 = myThread(2, 'thread_'+str(2), df2, lock_dict)\n",
    "thread3 = myThread(3, 'thread_'+str(3), df3, lock_dict)\n",
    "thread4 = myThread(4, 'thread_'+str(4), df4, lock_dict)\n",
    "\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "thread4.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "loaded_df = sqlContext.read.parquet('hdfs://localhost:9000/user/cloudray/NORA/partition_TEST.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(0=11, 1=12), Row(0=1, 1=2), Row(0=31, 1=32), Row(0=21, 1=22), Row(0=13, 1=14), Row(0=3, 1=4), Row(0=33, 1=34), Row(0=23, 1=24)]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_df.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(0=1, 1=2), Row(0=3, 1=4)]\n",
      "[Row(0=11, 1=12), Row(0=13, 1=14)]\n",
      "[Row(0=21, 1=22), Row(0=23, 1=24)]\n",
      "[Row(0=31, 1=32), Row(0=33, 1=34)]\n"
     ]
    }
   ],
   "source": [
    "print(df1.head(2))\n",
    "print(df2.head(2))\n",
    "print(df3.head(2))\n",
    "print(df4.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: <unlocked _thread.lock object at 0x7f7b892bed80>}\n"
     ]
    }
   ],
   "source": [
    "print(lock_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
