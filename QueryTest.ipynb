{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.sql(\"SELECT * FROM parquet.`hdfs://localhost:9000/user/cloudray/NORA_Test/merged`\")\n",
    "# df = spark.sql(\"SELECT * FROM parquet.`hdfs://localhost:9000/user/cloudray/QdTree_Test/merged`\")\n",
    "# df = spark.sql(\"SELECT * FROM parquet.`hdfs://localhost:9000/user/cloudray/NORA/merged`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.count())\n",
    "# print('= = = = = = = = = =')\n",
    "# print(df.columns)\n",
    "# print('= = = = = = = = = =')\n",
    "# print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----+---+----+-----------------+-------------------+----+\n",
      "|      _c0|     _c1| _c2|_c3| _c4|              _c5|                _c6| _c7|\n",
      "+---------+--------+----+---+----+-----------------+-------------------+----+\n",
      "|2421478.0|144996.0|18.0|3.0|43.0|         87762.57|               0.06|0.05|\n",
      "| 929378.0|150001.0|16.0|2.0|42.0|          44142.0|               0.05|0.03|\n",
      "| 943175.0|150003.0|18.0|1.0| 5.0|           5265.0|               0.01|0.04|\n",
      "| 976262.0|150002.0|17.0|1.0|18.0|          18936.0|               0.01|0.01|\n",
      "|2557920.0|150001.0|16.0|3.0| 2.0|           2102.0|               0.03|0.07|\n",
      "|2558913.0|150001.0|16.0|3.0|45.0|          47295.0|               0.08|0.06|\n",
      "|1875302.0|150000.0|15.0|3.0|15.0|          15750.0|               0.08|0.08|\n",
      "|1120736.0|144991.0|13.0|1.0|47.0|         95691.53|               0.03|0.06|\n",
      "| 440773.0|150001.0|16.0|2.0|10.0|          10510.0|               0.08|0.07|\n",
      "|1234853.0|144995.0|17.0|5.0|41.0|83639.58999999998|               0.03|0.05|\n",
      "|1290182.0|150003.0|18.0|7.0|35.0|          36855.0|0.08999999999999998|0.03|\n",
      "|2050435.0|150004.0|19.0|2.0|35.0|          36890.0|                0.0|0.08|\n",
      "|3612679.0|144997.0|19.0|5.0|37.0|         75553.63|                0.1|0.01|\n",
      "|3635655.0|144995.0|17.0|3.0|29.0|         59159.71|               0.07| 0.0|\n",
      "|3665889.0|150004.0|19.0|4.0| 6.0|           6324.0|               0.01|0.05|\n",
      "| 506561.0|150004.0|19.0|2.0|13.0|          13702.0|0.08999999999999998|0.08|\n",
      "|1343203.0|149998.0|13.0|4.0|11.0|         22527.89|               0.02|0.07|\n",
      "|1358240.0|150000.0|15.0|4.0| 4.0|           4200.0|0.08999999999999998|0.01|\n",
      "|2110470.0|149999.0|14.0|2.0|48.0|         98351.52|               0.06| 0.0|\n",
      "|3713474.0|144992.0|14.0|2.0|41.0|83516.58999999998|               0.08|0.05|\n",
      "+---------+--------+----+---+----+-----------------+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time:  0.4454672336578369\n"
     ]
    }
   ],
   "source": [
    "# test query time NORA\n",
    "start_time = time.time()\n",
    "# df = spark.sql(\"SELECT * FROM parquet.`hdfs://localhost:9000/user/cloudray/NORA_Test/merged` \\\n",
    "#                WHERE _c1 > 1000 AND _c1 < 9000 AND _c2 >= 114.4 AND _c2 <= 185.3\")\n",
    "df = spark.sql(\"SELECT * FROM parquet.`hdfs://localhost:9000/user/cloudray/NORA_Test/merged` \\\n",
    "               WHERE _c1 > 1000 AND _c1 < 9000 AND _c2 >= 114.4 AND _c2 <= 185.3\").collect()\n",
    "# df = spark.sql(\"SELECT * FROM parquet.`hdfs://localhost:9000/user/cloudray/NORA/merged` \\\n",
    "#                WHERE _c2 >= 114.4 AND _c2 <= 185.3\")\n",
    "# df = spark.sql(\"SELECT * FROM parquet.`hdfs://localhost:9000/user/cloudray/NORA/merged` \\\n",
    "#                WHERE _c2 >= 114.4 AND _c2 <= 185.3\").collect()\n",
    "end_time = time.time()\n",
    "print('execution time: ', end_time - start_time)\n",
    "\n",
    "# execution time:  0.08401131629943848    # 4 partition without collect\n",
    "# execution time:  0.47371959686279297    # 4 partition with collect\n",
    "\n",
    "# execution time:  0.11887288093566895    # 1K+ partition without collect\n",
    "# execution time:  1.857546329498291      # 1K+ partition with collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time:  1.179133415222168\n"
     ]
    }
   ],
   "source": [
    "# test query time Qd-Tree\n",
    "start_time = time.time()\n",
    "# df = spark.sql(\"SELECT * FROM parquet.`hdfs://localhost:9000/user/cloudray/QdTree_Test/merged` \\\n",
    "#                WHERE _c1 > 1000 AND _c1 < 9000 AND _c2 >= 114.4 AND _c2 <= 185.3  \")\n",
    "df = spark.sql(\"SELECT * FROM parquet.`hdfs://localhost:9000/user/cloudray/QdTree_Test/merged` \\\n",
    "               WHERE _c1 > 1000 AND _c1 < 9000 AND _c2 >= 114.4 AND _c2 <= 185.3  \").collect()\n",
    "# df = spark.sql(\"SELECT * FROM parquet.`hdfs://localhost:9000/user/cloudray/QdTree/merged` \\\n",
    "#                WHERE _c2 >= 114.4 AND _c2 <= 185.3\")\n",
    "# df = spark.sql(\"SELECT * FROM parquet.`hdfs://localhost:9000/user/cloudray/QdTree/merged` \\\n",
    "#                WHERE _c2 >= 114.4 AND _c2 <= 185.3\").collect()\n",
    "end_time = time.time()\n",
    "print('execution time: ', end_time - start_time)\n",
    "\n",
    "# execution time:  0.08320879936218262 # 1 partition without collect\n",
    "# execution time:  1.5957744121551514  # 1 partition with collect\n",
    "\n",
    "# execution time:  0.07                # 56 partition without collect\n",
    "# execution time:  1.309511423110962   # 56 partition with collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conclusion:\n",
    "# 1. Each partition should not be too small, or the meta data calculation time will domainate the cost\n",
    "# 2. The number of partitions (between 2 methods) should not vary too much, or the meta calculation time\n",
    "#    could take a large portion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
