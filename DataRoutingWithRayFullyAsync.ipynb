{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import rtree\n",
    "from rtree import index\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import threading\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([(\"spark.executor.memory\", \"8g\"),(\"spark.driver.memory\",\"8g\"),\n",
    "                           (\"spark.memory.offHeap.enabled\",True),(\"spark.memory.offHeap.size\",\"8g\")])\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kdnode_2_border(kdnode):\n",
    "    lower = [domain[0] for domain in kdnode[0]]\n",
    "    upper = [domain[1] for domain in kdnode[0]]\n",
    "    border = tuple(lower + upper) # non interleave\n",
    "    return border\n",
    "\n",
    "def load_partitions_from_file(path):\n",
    "    stretched_kdnodes = genfromtxt(path, delimiter=',')\n",
    "    num_dims = int(stretched_kdnodes[0,0])\n",
    "    kdnodes = []\n",
    "    for i in range(len(stretched_kdnodes)):\n",
    "        domains = [ [stretched_kdnodes[i,k+1],stretched_kdnodes[i,1+num_dims+k]] for k in range(num_dims) ]\n",
    "        row = [domains]\n",
    "        row.append(stretched_kdnodes[i,2*num_dims+1])\n",
    "        # to be compatible with qd-tree's partition, that do not have the last 4 attributes\n",
    "        if len(stretched_kdnodes[i]) > 2*num_dims+2:\n",
    "            row.append(stretched_kdnodes[i,-4])\n",
    "            row.append(stretched_kdnodes[i,-3])\n",
    "            row.append(stretched_kdnodes[i,-2])\n",
    "            row.append(stretched_kdnodes[i,-1])\n",
    "        kdnodes.append(row)\n",
    "    return kdnodes\n",
    "\n",
    "def process_chunk_row(row, used_dims, pidx, pid_data_dict, count, k):\n",
    "    if count[0] % 100000 == 0:\n",
    "        print('proces',k,'has routed',count[0],'rows')\n",
    "    count[0] += 1\n",
    "    row_numpy = row.to_numpy()\n",
    "    row_used_dims_list = row_numpy[used_dims].tolist()\n",
    "    row_border = tuple(row_used_dims_list+row_used_dims_list)\n",
    "    try:\n",
    "        pid = list(pidx.intersection(row_border))[0]\n",
    "    except:\n",
    "        print(row_border)\n",
    "    if pid in pid_data_dict:\n",
    "        pid_data_dict[pid]+=[row_numpy.tolist()]\n",
    "        #print('update dict..')\n",
    "    else:\n",
    "        pid_data_dict[pid]=[row_numpy.tolist()]\n",
    "        #print('initialize dict..')\n",
    "\n",
    "@ray.remote\n",
    "def process_chunk(chunk, used_dims, partition_path, k):\n",
    "    print(\"enter data routing process\", k, '..')    \n",
    "    pid_data_dict = {}\n",
    "    partitions = load_partitions_from_file(partition_path)\n",
    "    p = index.Property()\n",
    "    p.leaf_capacity = 32\n",
    "    p.index_capacity = 32\n",
    "    p.NearMinimumOverlaoFactor = 16\n",
    "    p.fill_factor = 0.8\n",
    "    p.overwrite = True\n",
    "    pidx = index.Index(properties = p)\n",
    "    for i in range(len(partitions)):\n",
    "        pidx.insert(i, kdnode_2_border(partitions[i]))\n",
    "    count = [0]\n",
    "    chunk.apply(lambda row: process_chunk_row(row, used_dims, pidx, pid_data_dict, count, k), axis=1)\n",
    "    dict_id = ray.put(pid_data_dict)\n",
    "    print(\"exit data routing process\", k, \".\")\n",
    "    return dict_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(base_dict, new_dict):\n",
    "    for key, val in new_dict.items():\n",
    "        if key in base_dict:\n",
    "            base_dict[key] += val\n",
    "        else:\n",
    "            base_dict[key] = val\n",
    "    new_dict.clear()\n",
    "\n",
    "def dump_dict_2_hdfs_simple(merged_dict, pq_writers, column_names, hdfs_path):\n",
    "    print('= = = start dumping in main thread = = =')\n",
    "    for pid, val in merged_dict.items():\n",
    "        #print(\"writing to pid:\",pid)\n",
    "        path = hdfs_path + 'partition_' + str(pid)+'.parquet'\n",
    "        pdf = pd.DataFrame(val, columns=column_names)\n",
    "        adf = pa.Table.from_pandas(pdf)\n",
    "        pq_writers[pid].write_table(table=adf)\n",
    "    print('= = = exit dumping = = =')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_writers(table_path, cols, col_names, partition_path, hdfs_path, fs):\n",
    "    \n",
    "    schema = None\n",
    "    for chunk in pd.read_table(table_path, delimiter='|', usecols=cols, names=col_names, chunksize=10):\n",
    "        schema = pa.Schema.from_pandas(chunk)\n",
    "        break\n",
    "    \n",
    "    stretched_kdnodes = genfromtxt(partition_path, delimiter=',')\n",
    "    num_nodes = len(stretched_kdnodes)\n",
    "    \n",
    "    pq_writers = {}\n",
    "    for i in range(num_nodes):\n",
    "        path = hdfs_path + 'partition_' + str(i)+'.parquet'\n",
    "        pq_writers[i] = pq.ParquetWriter(path, schema, fs)\n",
    "    \n",
    "    return pq_writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_parallel(table_path, partition_path, chunk_size, used_dims, hdfs_path, num_dims, num_process, hdfs_private_ip):\n",
    "    \n",
    "    begin_time = time.time()\n",
    "    \n",
    "    ray.init(num_cpus=num_process)\n",
    "    \n",
    "    # column names for pandas dataframe\n",
    "    cols = [i for i in range(num_dims)]\n",
    "    col_names = ['_c'+str(i) for i in range(num_dims)]\n",
    "    \n",
    "    # pyarrow parquent append\n",
    "    #pq_writers = {}\n",
    "    fs = pa.fs.HadoopFileSystem(hdfs_private_ip, port=9000, user='hdfs', replication=1)\n",
    "    pq_writers = prepare_writers(table_path, cols, col_names, partition_path, hdfs_path, fs)\n",
    "    \n",
    "    # chunks\n",
    "    chunk_count = 0\n",
    "    \n",
    "    # collect object refs\n",
    "    result_ids = []\n",
    "    last_batch_ids = []\n",
    "    \n",
    "    first_loop = True\n",
    "    \n",
    "    for chunk in pd.read_table(table_path, delimiter='|', usecols=cols, names=col_names, chunksize=chunk_size):\n",
    "        print('reading chunk: ', chunk_count)\n",
    "        \n",
    "        chunk_id = ray.put(chunk)\n",
    "        result_id = process_chunk.remote(chunk_id, used_dims, partition_path, chunk_count % num_process)\n",
    "        \n",
    "        del chunk_id\n",
    "        result_ids.append(result_id)\n",
    "        del result_id\n",
    "        \n",
    "        # after all process allocated a chunk, process and dump the data\n",
    "        if chunk_count % num_process == num_process - 1:\n",
    "            \n",
    "            if first_loop:\n",
    "                first_loop = False\n",
    "                last_batch_ids = result_ids.copy()\n",
    "                result_ids.clear()\n",
    "                continue\n",
    "            else:\n",
    "                print(\"= = = Process Dump For Previous Batch (main thread) = = =\")\n",
    "                base_dict = {}\n",
    "                while len(last_batch_ids):\n",
    "                    done_id, last_batch_ids = ray.wait(last_batch_ids)\n",
    "                    dict_id = ray.get(done_id[0])\n",
    "                    result_dict = ray.get(dict_id)\n",
    "                    merge_dict(base_dict, result_dict)\n",
    "                    #dump_dict_2_hdfs_simple(result_dict, pq_writers, col_names, hdfs_path)\n",
    "                dump_dict_2_hdfs_simple(base_dict, pq_writers, col_names, hdfs_path)\n",
    "                base_dict.clear()\n",
    "                print(\"= = = Finish Dump For Previous Batch (main thread) = = =\")\n",
    "                last_batch_ids = result_ids.copy()\n",
    "                result_ids.clear()\n",
    "                     \n",
    "            current_time = time.time()\n",
    "            time_elapsed = current_time - begin_time\n",
    "            print(\"= = = TOTAL PROCESSED SO FAR:\", (chunk_count+1) * chunk_size,\"ROWS. TIME SPENT:\", time_elapsed, \"SECONDS = = =\")\n",
    "                \n",
    "        chunk_count += 1\n",
    "        \n",
    "    # process the last few batches\n",
    "    print(\"= = = = = = Synchronization Barrier = = = = = =\")\n",
    "    base_dict = {}\n",
    "    while len(last_batch_ids):\n",
    "        done_id, last_batch_ids = ray.wait(last_batch_ids)\n",
    "        dict_id = ray.get(done_id[0])\n",
    "        result_dict = ray.get(dict_id)\n",
    "        merge_dict(base_dict, result_dict)\n",
    "        #dump_dict_2_hdfs_simple(result_dict, pq_writers, col_names, hdfs_path)\n",
    "    print(\"= = finish merged = =\")\n",
    "    dump_dict_2_hdfs_simple(base_dict, pq_writers, col_names, hdfs_path)\n",
    "    base_dict.clear()\n",
    "    last_batch_ids.clear()\n",
    "\n",
    "    base_dict = {}\n",
    "    while len(result_ids):\n",
    "        done_id, result_ids = ray.wait(result_ids)\n",
    "        dict_id = ray.get(done_id[0])\n",
    "        result_dict = ray.get(dict_id)\n",
    "        merge_dict(base_dict, result_dict)\n",
    "        #dump_dict_2_hdfs_simple(result_dict, pq_writers, col_names, hdfs_path)\n",
    "    print(\"= = finish merged = =\")\n",
    "    chunk_ids.clear() # clear up the references\n",
    "    result_ids.clear() # clear up the references\n",
    "    dump_dict_2_hdfs_simple(base_dict, pq_writers, col_names, hdfs_path) \n",
    "    base_dict.clear()\n",
    "    result_ids.clear()\n",
    "\n",
    "    for pid, writer in pq_writers.items():\n",
    "        writer.close()\n",
    "    \n",
    "    ray.shutdown()\n",
    "    \n",
    "    finish_time = time.time()\n",
    "    print('total data routing and persisting time: ', finish_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = Configuration (UBDA Cloud Centos) = = =\n",
    "scale_factor = 100\n",
    "\n",
    "table_base_path = '/media/datadrive1/TPCH/dbgen/'\n",
    "table_path = table_base_path + 'lineitem_' + str(scale_factor) + '.tbl'\n",
    "\n",
    "num_process = 8\n",
    "chunk_size = 3000000\n",
    "# 6M rows = about 1GB raw data\n",
    "\n",
    "num_dims = 16\n",
    "used_dims = [1,2]\n",
    "\n",
    "# base path of HDFS\n",
    "hdfs_private_ip = '192.168.6.62'\n",
    "hdfs_base_path = 'hdfs://192.168.6.62:9000/user/cloudray/'\n",
    "\n",
    "nora_hdfs = hdfs_base_path + 'NORA/scale' + str(scale_factor) + '/'\n",
    "qdtree_hdfs = hdfs_base_path + 'QdTree/scale' + str(scale_factor) + '/'\n",
    "kdtree_hdfs = hdfs_base_path + 'KDTree/scale' + str(scale_factor) + '/'\n",
    "\n",
    "# base path of Partition\n",
    "partition_base_path = '/home/centos/PartitionLayout/'\n",
    "\n",
    "nora_partition = partition_base_path + 'nora_partitions_' + str(scale_factor)\n",
    "qdtree_partition = partition_base_path + 'qdtree_partitions_' + str(scale_factor)\n",
    "kdtree_partition = partition_base_path + 'kdtree_partitions_' + str(scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-28 15:14:21,086\tINFO services.py:1164 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading chunk:  0\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m enter data routing process 0 ..\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 0 rows\n",
      "reading chunk:  1\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m enter data routing process 1 ..\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 300000 rows\n",
      "reading chunk:  2\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m enter data routing process 2 ..\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 300000 rows\n",
      "reading chunk:  3\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m enter data routing process 3 ..\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 300000 rows\n",
      "reading chunk:  4\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m enter data routing process 4 ..\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 300000 rows\n",
      "reading chunk:  5\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m enter data routing process 5 ..\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 1100000 rows\n",
      "reading chunk:  6\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m enter data routing process 6 ..\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 300000 rows\n",
      "reading chunk:  7\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 400000 rows\n",
      "= = = = = = Synchronization Barrier = = = = = =\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m enter data routing process 7 ..\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 2500000 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m proces 0 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m proces 1 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2016)\u001b[0m exit data routing process 0 .\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m proces 2 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2017)\u001b[0m exit data routing process 1 .\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 2300000 rows\n",
      "= = = start dumping in main thread = = =\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m proces 3 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2021)\u001b[0m exit data routing process 2 .\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 2400000 rows\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m proces 4 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2023)\u001b[0m exit data routing process 3 .\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m proces 5 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=2020)\u001b[0m exit data routing process 4 .\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m proces 6 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=2018)\u001b[0m exit data routing process 5 .\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 7 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=2022)\u001b[0m exit data routing process 6 .\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m exit data routing process 7 .\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "= = = exit dumping = = =\n",
      "= = finish merged = =\n",
      "= = = TOTAL PROCESSED SO FAR: 24000000 ROWS. TIME SPENT: 321.73661041259766 SECONDS = = =\n",
      "reading chunk:  8\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m enter data routing process 0 ..\n",
      "\u001b[2m\u001b[36m(pid=2019)\u001b[0m proces 0 has routed 0 rows\n"
     ]
    }
   ],
   "source": [
    "# = = = Execution = = =\n",
    "if __name__ == '__main__':\n",
    "    # batch_data_parallel(table_path, nora_partition, chunk_size, used_dims, nora_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "    # print('finish nora data routing..')\n",
    "    # batch_data_parallel(table_path, qdtree_partition, chunk_size, used_dims, qdtree_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "    # print('finish qdtree data routing..')\n",
    "    batch_data_parallel(table_path, kdtree_partition, chunk_size, used_dims, kdtree_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "    print('finish kdtree data routing..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ids = [1,2,3]\n",
    "\n",
    "last_batch_ids = results_ids\n",
    "results_ids.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
