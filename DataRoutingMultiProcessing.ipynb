{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import rtree\n",
    "from rtree import index\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from multiprocessing import Pool\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DRProcess import *\n",
    "from DDProcess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([(\"spark.executor.memory\", \"8g\"),(\"spark.driver.memory\",\"8g\"),\n",
    "                           (\"spark.memory.offHeap.enabled\",True),(\"spark.memory.offHeap.size\",\"8g\")])\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.memory.offHeap.size', '8g'),\n",
       " ('spark.driver.port', '45245'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', '10.0.2.15'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.id', 'local-1596809457619'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.memory', '8g'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.memory.offHeap.enabled', 'True'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DumpThread(threading.Thread):\n",
    "    def __init__(self, thread_id, name, parameters):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.thread_id = thread_id\n",
    "        self.name = name\n",
    "        self.parameters = parameters\n",
    "        \n",
    "    def run(self):\n",
    "        print('start dumping thread: ', self.thread_id, self.name)\n",
    "        start_index, end_index, pids, pid_data_dict, hdfs_path, column_names = self.parameters\n",
    "        for pid in pids[start_index: end_index]:\n",
    "            path = hdfs_path + 'partition_' + str(pid)+'.parquet'\n",
    "            pdf = pd.DataFrame(pid_data_dict[pid], columns=column_names)\n",
    "            df = sqlContext.createDataFrame(pdf)\n",
    "            df.write.mode('append').parquet(path)\n",
    "            pid_data_dict[pid] = []\n",
    "        print('exit dumping thread: ', self.thread_id, self.name)\n",
    "        \n",
    "def dump_dict_data_2_hdfs(pid_data_dicts, column_names, hdfs_path, num_threads = 8):\n",
    "    \n",
    "    # first merge all the dicts\n",
    "    base_dict = pid_data_dicts[0]\n",
    "    for k in range(1, len(pid_data_dicts)):\n",
    "        for key, val in pid_data_dicts[k].items():\n",
    "            if key in base_dict:\n",
    "                base_dict[key] += val\n",
    "            else:\n",
    "                base_dict.update({key:val})\n",
    "        pid_data_dicts[k].clear()\n",
    "    \n",
    "    if num_threads == 1:\n",
    "        print('start dumping single thread (main)')\n",
    "        pids = list(base_dict.keys())\n",
    "        for pid in pids:\n",
    "            path = hdfs_path + 'partition_' + str(pid)+'.parquet'\n",
    "            pdf = pd.DataFrame(base_dict[pid], columns=column_names)\n",
    "            df = sqlContext.createDataFrame(pdf)\n",
    "            df.write.mode('append').parquet(path)\n",
    "            base_dict[pid] = []\n",
    "        print('finish dumping single thread (main)')\n",
    "    \n",
    "    else:\n",
    "        # apply multi-threading to save\n",
    "        pids = list(base_dict.keys())\n",
    "        step = int(len(pids) / num_threads) + 1\n",
    "        threads = []\n",
    "        for i in range(num_threads):\n",
    "            start_index = i * step\n",
    "            end_index = (i+1) * step\n",
    "            parameters = [start_index, end_index, pids, base_dict, hdfs_path, column_names]\n",
    "            thread = DumpThread(i, 'dump_thread_'+str(i), parameters)\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "            if start_index >= len(pids):\n",
    "                break   \n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "# used for multi-process wirting\n",
    "def merge_dicts(pid_data_dicts, num_process):\n",
    "    base_dict = pid_data_dicts[0]\n",
    "    for k in range(1, len(pid_data_dicts)):\n",
    "        for key, val in pid_data_dicts[k].items():\n",
    "            if key in base_dict:\n",
    "                base_dict[key] += val\n",
    "            else:\n",
    "                base_dict.update({key:val})\n",
    "        pid_data_dicts[k].clear()\n",
    "    \n",
    "    # re allocate to non-overlap dicts\n",
    "    pids = list(base_dict.keys())\n",
    "    step = int(len(pids) / num_process) + 1\n",
    "    non_overlap_dicts = [{} for i in range(num_process)]\n",
    "    \n",
    "    for key, val in base_dict.items():\n",
    "        dict_index = key // step\n",
    "        non_overlap_dicts[dict_index][key] = val\n",
    "        \n",
    "    return non_overlap_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_parallel(table_path, partition_path, chunk_size, used_dims, hdfs_path, \n",
    "                        num_dims, dump_threshold = 1000000, num_process = 8):\n",
    "    \n",
    "    begin_time = time.time()\n",
    "    \n",
    "    col_names = ['_c'+str(i) for i in range(num_dims)]\n",
    "    cols = [i for i in range(num_dims)]\n",
    "    \n",
    "    pid_data_dicts = []\n",
    "    for i in range(num_process):\n",
    "        pid_data_dicts.append({})\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    count = 0\n",
    "    epochs = 0\n",
    "    processed_data = 0\n",
    "    pool = Pool(processes = num_process) # the pool should be reused, or incur memory leak!\n",
    "    pids_each_process = [set() for k in range(num_process)] # used for final merge\n",
    "    \n",
    "    #for chunk in pd.read_table(table_path, delimiter='|', usecols=cols, names=col_names, chunksize=chunk_size):\n",
    "    for chunk in pd.read_csv(table_path, usecols=cols, names=col_names, chunksize=chunk_size):\n",
    "        print('current chunk: ', count)\n",
    "        chunks.append(chunk)\n",
    "        if count % num_process == num_process - 1:\n",
    "            paras = [[chunks[k], used_dims, partition_path, pid_data_dicts[k]] for k in range(num_process)]\n",
    "            pid_data_dicts = pool.map(process_chunk, [para for para in paras])\n",
    "            print('===================================================')\n",
    "            chunks = []\n",
    "            processed_data += chunk_size * num_process\n",
    "            \n",
    "            # dump data to file\n",
    "            if processed_data >= dump_threshold:\n",
    "                # parquet write is not thread safe, avoid concurent write\n",
    "                pid_data_dicts = merge_dicts(pid_data_dicts, num_process) # make it non-overlap\n",
    "                for k in range(num_process):\n",
    "                    pids_each_process[k].update(list(pid_data_dicts[k].keys()))\n",
    "                paras = [[epochs, pid_data_dicts[k], col_names, hdfs_path] for k in range(num_process)]\n",
    "                pool.map(dump_data, [para for para in paras])\n",
    "                #dump_dict_data_2_hdfs(pid_data_dicts, col_names, hdfs_path) # multi-thread\n",
    "                processed_data = 0\n",
    "                epochs += 1\n",
    "                for i in range(num_process):\n",
    "                    pid_data_dicts[i].clear()\n",
    "        count += 1\n",
    "        \n",
    "    dict_size = [len(pid_data_dicts[i]) for i in range(num_process)]\n",
    "    print('after exit, chunks size: ', len(chunks))\n",
    "    print('after exit, each dict size: ', dict_size)\n",
    "    # process the last batch\n",
    "    if len(chunks) != 0:\n",
    "        paras = [[chunks[k], used_dims, partition_path, pid_data_dicts[k]] for k in range(len(chunks))]\n",
    "        pid_data_dicts[0:len(chunks)] = pool.map(process_chunk, [para for para in paras])\n",
    "    \n",
    "    \n",
    "    dict_size = [len(pid_data_dicts[i]) for i in range(num_process)]\n",
    "    print('after last chunk, each dict size: ', dict_size)\n",
    "    \n",
    "    if len(pid_data_dicts[0]) != 0:\n",
    "        pid_data_dicts = merge_dicts(pid_data_dicts, num_process) # make it non-overlap\n",
    "        paras = [[epochs, pid_data_dicts[k], col_names, hdfs_path] for k in range(num_process)]\n",
    "        pool.map(dump_data, [para for para in paras])\n",
    "        #dump_dict_data_2_hdfs(pid_data_dicts, col_names, hdfs_path)\n",
    "        for k in range(num_process):\n",
    "            pids_each_process[k].update(list(pid_data_dicts[k].keys()))\n",
    "    \n",
    "    pid_data_dicts.clear() # release memory\n",
    "    \n",
    "    # final merge\n",
    "    epochs += 1\n",
    "    paras = [[epochs, pids_each_process[k], hdfs_path] for k in range(num_process)]\n",
    "    pool.map(merge_parquets, [para for para in paras])\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    finish_time = time.time()\n",
    "    print('total data routing and persisting time: ', finish_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current chunk:  0\n",
      "current chunk:  1\n",
      "current chunk:  2\n",
      "current chunk:  3\n",
      "current chunk:  4\n",
      "current chunk:  5\n",
      "current chunk:  6\n",
      "current chunk:  7\n",
      "===================================================\n",
      "current chunk:  8\n",
      "current chunk:  9\n",
      "current chunk:  10\n",
      "current chunk:  11\n",
      "current chunk:  12\n",
      "current chunk:  13\n",
      "current chunk:  14\n",
      "current chunk:  15\n",
      "===================================================\n",
      "current chunk:  16\n",
      "current chunk:  17\n",
      "current chunk:  18\n",
      "current chunk:  19\n",
      "current chunk:  20\n",
      "current chunk:  21\n",
      "current chunk:  22\n",
      "current chunk:  23\n",
      "===================================================\n",
      "current chunk:  24\n",
      "current chunk:  25\n",
      "current chunk:  26\n",
      "current chunk:  27\n",
      "current chunk:  28\n",
      "current chunk:  29\n",
      "current chunk:  30\n",
      "current chunk:  31\n",
      "===================================================\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "current chunk:  32\n",
      "current chunk:  33\n",
      "current chunk:  34\n",
      "current chunk:  35\n",
      "current chunk:  36\n",
      "current chunk:  37\n",
      "current chunk:  38\n",
      "current chunk:  39\n",
      "===================================================\n",
      "current chunk:  40\n",
      "current chunk:  41\n",
      "current chunk:  42\n",
      "current chunk:  43\n",
      "current chunk:  44\n",
      "current chunk:  45\n",
      "current chunk:  46\n",
      "current chunk:  47\n",
      "===================================================\n",
      "current chunk:  48\n",
      "current chunk:  49\n",
      "current chunk:  50\n",
      "current chunk:  51\n",
      "current chunk:  52\n",
      "current chunk:  53\n",
      "current chunk:  54\n",
      "current chunk:  55\n",
      "===================================================\n",
      "current chunk:  56\n",
      "current chunk:  57\n",
      "current chunk:  58\n",
      "current chunk:  59\n",
      "current chunk:  60\n",
      "current chunk:  61\n",
      "current chunk:  62\n",
      "current chunk:  63\n",
      "===================================================\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "current chunk:  64\n",
      "current chunk:  65\n",
      "current chunk:  66\n",
      "current chunk:  67\n",
      "current chunk:  68\n",
      "current chunk:  69\n",
      "current chunk:  70\n",
      "current chunk:  71\n",
      "===================================================\n",
      "current chunk:  72\n",
      "current chunk:  73\n",
      "current chunk:  74\n",
      "current chunk:  75\n",
      "current chunk:  76\n",
      "current chunk:  77\n",
      "current chunk:  78\n",
      "current chunk:  79\n",
      "===================================================\n",
      "current chunk:  80\n",
      "current chunk:  81\n",
      "current chunk:  82\n",
      "current chunk:  83\n",
      "current chunk:  84\n",
      "current chunk:  85\n",
      "current chunk:  86\n",
      "current chunk:  87\n",
      "===================================================\n",
      "current chunk:  88\n",
      "current chunk:  89\n",
      "current chunk:  90\n",
      "current chunk:  91\n",
      "current chunk:  92\n",
      "current chunk:  93\n",
      "current chunk:  94\n",
      "current chunk:  95\n",
      "===================================================\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "current chunk:  96\n",
      "current chunk:  97\n",
      "current chunk:  98\n",
      "current chunk:  99\n",
      "current chunk:  100\n",
      "current chunk:  101\n",
      "current chunk:  102\n",
      "current chunk:  103\n",
      "===================================================\n",
      "current chunk:  104\n",
      "current chunk:  105\n",
      "current chunk:  106\n",
      "current chunk:  107\n",
      "current chunk:  108\n",
      "current chunk:  109\n",
      "current chunk:  110\n",
      "current chunk:  111\n",
      "===================================================\n",
      "current chunk:  112\n",
      "current chunk:  113\n",
      "current chunk:  114\n",
      "current chunk:  115\n",
      "current chunk:  116\n",
      "current chunk:  117\n",
      "current chunk:  118\n",
      "current chunk:  119\n",
      "===================================================\n",
      "after exit, chunks size:  0\n",
      "after exit, each dict size:  [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "after last chunk, each dict size:  [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit dumping process\n",
      "exit merge process\n",
      "exit merge process\n",
      "exit merge process\n",
      "exit merge process\n",
      "exit merge process\n",
      "exit merge process\n",
      "exit merge process\n",
      "exit merge process\n",
      "total data routing and persisting time:  290.816454410553\n"
     ]
    }
   ],
   "source": [
    "# = = = Execution = = =\n",
    "\n",
    "num_process = 8\n",
    "chunk_size = 100000 \n",
    "dump_threshold = 3200000\n",
    "\n",
    "num_dims = 8\n",
    "used_dims = [1,2]\n",
    "\n",
    "table_path = '/home/cloudray/Downloads/TPCH_12M_8Field.csv'\n",
    "# table_path = '/home/cloudray/TPCH/2.18.0_rc2/dbgen/lineitem.tbl'\n",
    "\n",
    "# partition_path = '/home/cloudray/NORA_Partitions/nora_partitions'\n",
    "# partition_path = '/home/cloudray/NORA_Partitions/qd_tree_partitions'\n",
    "\n",
    "# hdfs_path = 'hdfs://localhost:9000/user/cloudray/NORA/'\n",
    "# hdfs_path = 'hdfs://localhost:9000/user/cloudray/QdTree/'\n",
    "\n",
    "\n",
    "# partition_path = '/home/cloudray/NORA_Partitions/nora_test'\n",
    "partition_path = '/home/cloudray/NORA_Partitions/qd_tree_test'\n",
    "\n",
    "# hdfs_path = 'hdfs://localhost:9000/user/cloudray/NORA_Test/'\n",
    "hdfs_path = 'hdfs://localhost:9000/user/cloudray/QdTree_Test/'\n",
    "\n",
    "batch_data_parallel(table_path, partition_path, chunk_size, used_dims, \n",
    "                    hdfs_path, num_dims, dump_threshold, num_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total data routing and persisting time:  1398.6325314044952 # Nora\n",
    "# total data routing and persisting time:  1193.6831953525543 # QdTree\n",
    "# total data routing and persisting time:  1245.9338216781616 # QdTree Test#\n",
    "# Try multi-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pid_data_dicts = [{1:2,2:3},{2:4,3:6}]\n",
    "# pids_each_process = [set(), set()]\n",
    "# for k in range(2):\n",
    "#     pids_each_process[k].update(list(pid_data_dicts[k].keys()))\n",
    "# pids_each_process\n",
    "# pid_data_dicts = [{4:2,5:3},{6:4,7:6}]\n",
    "# for k in range(2):\n",
    "#     pids_each_process[k].update(list(pid_data_dicts[k].keys()))\n",
    "# pids_each_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rtree import index\n",
    "\n",
    "# partitions = load_partitions_from_file('/home/cloudray/NORA_Partitions/nora_test')\n",
    "# p = index.Property()\n",
    "# p.leaf_capacity = 32\n",
    "# p.index_capacity = 32\n",
    "# p.NearMinimumOverlaoFactor = 16\n",
    "# p.fill_factor = 0.8\n",
    "# p.overwrite = True\n",
    "# pidx = index.Index(properties = p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kdnode_2_border(kdnode):\n",
    "#     lower = [domain[0] for domain in kdnode[0]]\n",
    "#     upper = [domain[1] for domain in kdnode[0]]\n",
    "#     border = tuple(lower + upper) # non interleave\n",
    "#     return border\n",
    "\n",
    "# def load_partitions_from_file(path):\n",
    "#     stretched_kdnodes = genfromtxt(path, delimiter=',')\n",
    "#     num_dims = int(stretched_kdnodes[0,0])\n",
    "#     kdnodes = []\n",
    "#     for i in range(len(stretched_kdnodes)):\n",
    "#         domains = [ [stretched_kdnodes[i,k+1],stretched_kdnodes[i,1+num_dims+k]] for k in range(num_dims) ]\n",
    "#         row = [domains]\n",
    "#         row.append(stretched_kdnodes[i,2*num_dims+1])\n",
    "#         # to be compatible with qd-tree's partition, that do not have the last 4 attributes\n",
    "#         if len(stretched_kdnodes[i]) > 2*num_dims+2:\n",
    "#             row.append(stretched_kdnodes[i,-4])\n",
    "#             row.append(stretched_kdnodes[i,-3])\n",
    "#             row.append(stretched_kdnodes[i,-2])\n",
    "#             row.append(stretched_kdnodes[i,-1])\n",
    "#         kdnodes.append(row)\n",
    "#     return kdnodes\n",
    "\n",
    "# partitions = load_partitions_from_file('/home/cloudray/NORA_Partitions/nora_test')\n",
    "# for i in range(len(partitions)):\n",
    "#     pidx.insert(i, kdnode_2_border(partitions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_border = (134619.0, 14620.0, 134619.0, 14620.0)\n",
    "# pid = list(pidx.intersection(row_border))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
