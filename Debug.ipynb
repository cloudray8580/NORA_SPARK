{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import rtree\n",
    "from rtree import index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_upper_check(kdnodes):\n",
    "    for i in range(len(kdnodes)):\n",
    "        for j in range(len(kdnodes[i][0])):\n",
    "            if kdnodes[i][0][j][0] > kdnodes[i][0][j][1]:\n",
    "                print('found invalid kdnode',i,kdnodes[i])\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partitions_from_file(path):\n",
    "    '''\n",
    "    the loaded stretched_kdnodes: [num_dims, l1,l2,...,ln, u1,u2,...,un, size, id, pid, left_child,id, right_child_id]\n",
    "    '''\n",
    "    stretched_kdnodes = genfromtxt(path, delimiter=',')\n",
    "    num_dims = int(stretched_kdnodes[0,0])\n",
    "    kdnodes = []\n",
    "    \n",
    "    for i in range(len(stretched_kdnodes)):\n",
    "        domains = [ [stretched_kdnodes[i,k+1],stretched_kdnodes[i,1+num_dims+k]] for k in range(num_dims) ]\n",
    "        row = [domains]\n",
    "        row.append(stretched_kdnodes[i,2*num_dims+1])\n",
    "        \n",
    "        # to be compatible with qd-tree's partition, that do not have the last 4 attributes\n",
    "        if len(stretched_kdnodes[i]) > 2*num_dims+2:\n",
    "            row.append(stretched_kdnodes[i,-4])\n",
    "            row.append(stretched_kdnodes[i,-3])\n",
    "            row.append(stretched_kdnodes[i,-2])\n",
    "            row.append(stretched_kdnodes[i,-1])\n",
    "    \n",
    "        kdnodes.append(row)\n",
    "    \n",
    "    return kdnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Utils import *\n",
    "# path = 'C:/Users/Cloud/iCloudDrive/HUAWEI_LKD/HDFSExperiment/generated_partitions/nora_partitions'\n",
    "# path = 'C:/Users/Cloud/iCloudDrive/HUAWEI_LKD/HDFSExperiment/generated_partitions/qd_tree_partitions'\n",
    "\n",
    "# path = '/home/cloudray/NORA_Partitions/nora_partitions'\n",
    "path = '/home/cloudray/NORA_Partitions/qd_tree_partitions'\n",
    "\n",
    "saved_partitions = load_partitions_from_file(path)\n",
    "lower_upper_check(saved_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '/home/cloudray/Downloads/TPCH_12M_8Field.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = genfromtxt(raw_data_path, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if every point in dataset can find a partition that holds it\n",
    "# for i in range(len(dataset)):\n",
    "#     if i% 100 == 0:\n",
    "#         print('processing..',i)\n",
    "#     find_tag = False\n",
    "#     for j in range(saved_partitions):\n",
    "#         if dataset[i][1] >= saved_partitions[j][0][0][0] and dataset[i][1] <= saved_partitions[j][0][0][1]\\\n",
    "#         and dataset[i][2] >= saved_partitions[j][0][1][0] and dataset[i][2] <= saved_partitions[j][0][1][1]:\n",
    "#             find_tag = True\n",
    "#             break\n",
    "#     if find_tag == False:\n",
    "#         print('no partition that can hold the',i,'point: ',dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_count = 0\n",
    "# for chunk in pd.read_csv(raw_data_path, chunksize=10000):\n",
    "#     print('current chunk: ', chunk_count)\n",
    "#     dataset = chunk.to_numpy()\n",
    "#     for i in range(len(dataset)):\n",
    "#         find_tag = False\n",
    "#         for j in range(len(saved_partitions)):\n",
    "#             if dataset[i][1] >= saved_partitions[j][0][0][0] and dataset[i][1] <= saved_partitions[j][0][0][1]\\\n",
    "#             and dataset[i][2] >= saved_partitions[j][0][1][0] and dataset[i][2] <= saved_partitions[j][0][1][1]:\n",
    "#                 find_tag = True\n",
    "#                 break\n",
    "#         if find_tag == False:\n",
    "#             print('no partition that can hold the',i,'point: ',dataset[i])\n",
    "#             break\n",
    "#     chunk_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kdnode_2_border(kdnode):\n",
    "    lower = [domain[0] for domain in kdnode[0]]\n",
    "    upper = [domain[1] for domain in kdnode[0]]\n",
    "    border = tuple(lower + upper) # non interleave\n",
    "    return border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test after indexing partitions by Rtree, is there any record that cannot find a partition to hold?\n",
    "\n",
    "p = index.Property()\n",
    "p.leaf_capacity = 100 # cannot be less than 100, indicate the maximum capacity\n",
    "p.fill_factor = 0.5\n",
    "p.overwrite = True\n",
    "\n",
    "partition_index = index.Index(properties = p)\n",
    "for i in range(len(saved_partitions)):\n",
    "    #partition_index.insert(int(partitions[i][-4]), kdnode_2_border(partitions[i]))\n",
    "    partition_index.insert(i, kdnode_2_border(saved_partitions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtree.index.Index(bounds=[1.0, 1.0, 400000.0, 20000.0], size=53)\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(partition_index)\n",
    "print(partition_index.get_size())\n",
    "print(len(saved_partitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n",
      "[(0, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52], [1.0, 1.0, 400000.0, 20000.0])]\n",
      "[[[240110.55543950442, 278102.008168012], [10252.013667268735, 14843.77380863712]], 263925.0]\n"
     ]
    }
   ],
   "source": [
    "overlap_pids = list(partition_index.intersection((275983.0, 11023.0, 275983.0, 11023.0)))\n",
    "print(overlap_pids)\n",
    "print(partition_index.leaves())\n",
    "print(saved_partitions[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "idx_copy = copy.deepcopy(partition_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current chunk:  0\n",
      "current chunk:  1\n",
      "current chunk:  2\n",
      "current chunk:  3\n",
      "current chunk:  4\n",
      "current chunk:  5\n",
      "current chunk:  6\n",
      "current chunk:  7\n",
      "current chunk:  8\n",
      "current chunk:  9\n",
      "current chunk:  10\n",
      "current chunk:  11\n",
      "current chunk:  12\n",
      "current chunk:  13\n",
      "current chunk:  14\n",
      "current chunk:  15\n",
      "current chunk:  16\n",
      "current chunk:  17\n",
      "current chunk:  18\n",
      "current chunk:  19\n",
      "current chunk:  20\n",
      "current chunk:  21\n",
      "current chunk:  22\n",
      "current chunk:  23\n",
      "current chunk:  24\n",
      "current chunk:  25\n",
      "current chunk:  26\n",
      "current chunk:  27\n",
      "current chunk:  28\n",
      "current chunk:  29\n",
      "current chunk:  30\n",
      "current chunk:  31\n",
      "current chunk:  32\n",
      "current chunk:  33\n",
      "current chunk:  34\n",
      "current chunk:  35\n",
      "current chunk:  36\n",
      "current chunk:  37\n",
      "current chunk:  38\n",
      "current chunk:  39\n",
      "current chunk:  40\n",
      "current chunk:  41\n",
      "current chunk:  42\n",
      "current chunk:  43\n",
      "current chunk:  44\n",
      "current chunk:  45\n",
      "current chunk:  46\n",
      "current chunk:  47\n",
      "current chunk:  48\n",
      "current chunk:  49\n",
      "current chunk:  50\n",
      "current chunk:  51\n",
      "current chunk:  52\n",
      "current chunk:  53\n",
      "current chunk:  54\n",
      "current chunk:  55\n",
      "current chunk:  56\n",
      "current chunk:  57\n",
      "current chunk:  58\n",
      "current chunk:  59\n",
      "current chunk:  60\n",
      "current chunk:  61\n",
      "current chunk:  62\n",
      "current chunk:  63\n",
      "current chunk:  64\n",
      "current chunk:  65\n",
      "current chunk:  66\n",
      "current chunk:  67\n",
      "current chunk:  68\n",
      "current chunk:  69\n",
      "current chunk:  70\n",
      "current chunk:  71\n",
      "current chunk:  72\n",
      "current chunk:  73\n",
      "current chunk:  74\n",
      "current chunk:  75\n",
      "current chunk:  76\n",
      "current chunk:  77\n",
      "current chunk:  78\n",
      "current chunk:  79\n",
      "current chunk:  80\n",
      "current chunk:  81\n",
      "current chunk:  82\n",
      "current chunk:  83\n",
      "current chunk:  84\n",
      "current chunk:  85\n",
      "current chunk:  86\n",
      "current chunk:  87\n",
      "current chunk:  88\n",
      "current chunk:  89\n",
      "current chunk:  90\n",
      "current chunk:  91\n",
      "current chunk:  92\n",
      "current chunk:  93\n",
      "current chunk:  94\n",
      "current chunk:  95\n",
      "current chunk:  96\n",
      "current chunk:  97\n",
      "current chunk:  98\n",
      "current chunk:  99\n",
      "current chunk:  100\n",
      "current chunk:  101\n",
      "current chunk:  102\n",
      "current chunk:  103\n",
      "current chunk:  104\n",
      "current chunk:  105\n",
      "current chunk:  106\n",
      "current chunk:  107\n",
      "current chunk:  108\n",
      "current chunk:  109\n",
      "current chunk:  110\n",
      "current chunk:  111\n",
      "current chunk:  112\n",
      "current chunk:  113\n",
      "current chunk:  114\n",
      "current chunk:  115\n",
      "current chunk:  116\n",
      "current chunk:  117\n",
      "current chunk:  118\n",
      "current chunk:  119\n"
     ]
    }
   ],
   "source": [
    "chunk_count = 0\n",
    "for chunk in pd.read_csv(raw_data_path, chunksize=100000):\n",
    "    print('current chunk: ', chunk_count)\n",
    "    dataset = chunk.to_numpy()\n",
    "    for i in range(len(dataset)):\n",
    "        point_border = (dataset[i,1], dataset[i,2], dataset[i,1], dataset[i,2])\n",
    "        overlap_pids = list(partition_index.intersection(point_border))\n",
    "        if len(overlap_pids) == 0:\n",
    "            print('no partition that can hold the',i,'point: ',dataset[i])\n",
    "            break\n",
    "    chunk_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
