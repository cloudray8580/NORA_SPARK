{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import rtree\n",
    "from rtree import index\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider using a partition_rtree_index to find the corresponding partition\n",
    "# this is single thread\n",
    "def record_2_border(row, used_dims):\n",
    "    '''\n",
    "    row should be a pandas row, i.e., a point\n",
    "    border is the border required in rtree index\n",
    "    '''\n",
    "    row_used_dims = row.iloc[:,used_dims]\n",
    "    row_list = row_used_dims.values.tolist()[0]\n",
    "    return tuple(row_list + row_list)\n",
    "   \n",
    "def route_data_2_partition(dataset, used_dims, partition_index, hdfs_path, lock_dict, print_execution_time=False):\n",
    "    '''\n",
    "    parameters:\n",
    "    @dataset: should be in the form of pandas dataframe here\n",
    "    @partition_index: the index of partitions\n",
    "    @column_names: a full list of the column names of the table, like ['_c0','_c1','_c2']\n",
    "    '''   \n",
    "    pid_pdf_dict = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        \n",
    "        record = dataset[i:i+1] # row shape = (1, n_dims)\n",
    "        point_border = record_2_border(record, used_dims)\n",
    "        overlap_pids = list(partition_index.intersection(point_border)) # should only contains 1\n",
    "        #print('in route func, partition index size: ', partition_index.get_size())\n",
    "        if len(overlap_pids) == 0:\n",
    "            #print('error: no partition that can hold this record')\n",
    "            #print('record: ',record)\n",
    "            print('error point_border: ',point_border)\n",
    "            print('error partition index size: ', partition_index.get_size())\n",
    "        pid = overlap_pids[0]\n",
    "        \n",
    "        # assign this record to the corresponding partition\n",
    "        if pid in pid_pdf_dict:\n",
    "            #pid_pdf_dict[pid] = pid_pdf_dict[pid].append(record) # must return, cannot replace\n",
    "            pid_pdf_dict[pid] = pd.concat([pid_pdf_dict[pid], record]) # a little bit faster\n",
    "        else:\n",
    "            pid_pdf_dict.update({pid:record})\n",
    "    \n",
    "    routing_time = time.time()\n",
    "    \n",
    "    # persist them in HDFS\n",
    "    for pid, pdf in pid_pdf_dict.items():\n",
    "        partition_name = 'partition_' + str(pid)+'.parquet'\n",
    "        path = hdfs_path + partition_name\n",
    "        #pdf.columns = column_names\n",
    "        df = sqlContext.createDataFrame(pdf)\n",
    "        lock_dict[pid].acquire() # lock\n",
    "        df.write.mode('append').parquet(path)\n",
    "        lock_dict[pid].release() # unlock\n",
    "        \n",
    "    persist_time = time.time()\n",
    "    \n",
    "    if print_execution_time:\n",
    "        print(pid_pdf_dict)\n",
    "        print('data routing time: ', routing_time-start_time)\n",
    "        print('data persist time: ', persist_time-routing_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_data(raw_data_path, chunk_size, used_dims, partition_index, column_names, hdfs_path):\n",
    "#     begin_time = time.time()\n",
    "    \n",
    "#     chunk_count = 0\n",
    "#     for chunk in pd.read_csv(raw_data_path, chunksize=chunk_size):\n",
    "#         print('current chunk: ', chunk_count)\n",
    "#         chunk.columns = column_names\n",
    "#         route_data_2_partition(chunk, used_dims, partition_index, hdfs_path)\n",
    "#         chunk_count += 1\n",
    "    \n",
    "#     finish_time = time.time()\n",
    "#     print('total data routing and persisting time: ', finish_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try multi-thread\n",
    "import threading\n",
    "\n",
    "class myThread(threading.Thread):\n",
    "    def __init__(self, thread_id, name, counter, parameters, lock_dict):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.thread_id = thread_id\n",
    "        self.name = name\n",
    "        self.counter = counter\n",
    "        self.parameters = parameters\n",
    "        self.lock_dict = lock_dict\n",
    "        #print('in thread init, pidx size: ', parameters[2].get_size())\n",
    "        #print('in thread init, self.pidx size: ', self.parameters[2].get_size())\n",
    "        \n",
    "    def run(self):\n",
    "        print('start thread: ',self.thread_id, self.name)\n",
    "        chunk, used_dims, partition_index, hdfs_path = self.parameters\n",
    "        #print('in thread run, pidx size: ', partition_index.get_size())\n",
    "        route_data_2_partition(chunk, used_dims, partition_index, hdfs_path, self.lock_dict)\n",
    "        print('exit thread: ',self.thread_id, self.name)\n",
    "\n",
    "\n",
    "def batch_data_parallel(raw_data_path, chunk_size, used_dims, partition_index, column_names, \n",
    "                        hdfs_path, lock_dict, max_threads = 8):\n",
    "    \n",
    "    begin_time = time.time()\n",
    "    \n",
    "    count = 0\n",
    "    threads = []\n",
    "    for chunk in pd.read_csv(raw_data_path, chunksize=chunk_size):\n",
    "        print('current chunk: ', count)\n",
    "        tid = count % max_threads\n",
    "        chunk.columns = column_names\n",
    "        #print('in batch func, pidx size: ', partition_index.get_size())\n",
    "        parameters = [chunk, used_dims, partition_index[tid], hdfs_path]\n",
    "        #print('in batch func para, pidx size: ', parameters[2].get_size())\n",
    "        thread = myThread(tid, 'thread_'+str(tid)+'_'+str(count), count, parameters, lock_dict)\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "        count += 1\n",
    "        \n",
    "        if tid == max_threads-1:\n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            threads = []\n",
    "            print('===================================================')\n",
    "            \n",
    "    finish_time = time.time()\n",
    "    print('total data routing and persisting time: ', finish_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kdnode_2_border(kdnode):\n",
    "    lower = [domain[0] for domain in kdnode[0]]\n",
    "    upper = [domain[1] for domain in kdnode[0]]\n",
    "    border = tuple(lower + upper) # non interleave\n",
    "    return border\n",
    "\n",
    "def load_partitions_from_file(path):\n",
    "    '''\n",
    "    the loaded stretched_kdnodes: [num_dims, l1,l2,...,ln, u1,u2,...,un, size, id, pid, left_child,id, right_child_id]\n",
    "    '''\n",
    "    stretched_kdnodes = genfromtxt(path, delimiter=',')\n",
    "    num_dims = int(stretched_kdnodes[0,0])\n",
    "    kdnodes = []\n",
    "    \n",
    "    for i in range(len(stretched_kdnodes)):\n",
    "        domains = [ [stretched_kdnodes[i,k+1],stretched_kdnodes[i,1+num_dims+k]] for k in range(num_dims) ]\n",
    "        row = [domains]\n",
    "        row.append(stretched_kdnodes[i,2*num_dims+1])\n",
    "        \n",
    "        # to be compatible with qd-tree's partition, that do not have the last 4 attributes\n",
    "        if len(stretched_kdnodes[i]) > 2*num_dims+2:\n",
    "            row.append(stretched_kdnodes[i,-4])\n",
    "            row.append(stretched_kdnodes[i,-3])\n",
    "            row.append(stretched_kdnodes[i,-2])\n",
    "            row.append(stretched_kdnodes[i,-1])\n",
    "    \n",
    "        kdnodes.append(row)\n",
    "    \n",
    "    return kdnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_routing(data_path, partition_path, num_threads,\n",
    "                chunk_size, used_dims, column_names, hdfs_path):\n",
    "    \n",
    "    partitions = load_partitions_from_file(partition_path)\n",
    "    lock_dict = {}\n",
    "    \n",
    "    p = index.Property()\n",
    "    p.leaf_capacity = 100 # cannot be less than 100, indicate the maximum capacity\n",
    "    p.fill_factor = 0.5\n",
    "    p.overwrite = True\n",
    "    \n",
    "    pidxs = [] # the rtree index has problem in mutli-threading, create an index for each thread\n",
    "    for k in range(num_threads):\n",
    "        partition_index = index.Index(properties = p)\n",
    "        for i in range(len(partitions)):\n",
    "            # qd-tree do not have this\n",
    "            #partition_index.insert(int(partitions[i][-4]), kdnode_2_border(partitions[i])) \n",
    "            partition_index.insert(i, kdnode_2_border(partitions[i]))\n",
    "            lock_dict.update({i:threading.Lock()})\n",
    "        pidxs.append(partition_index)\n",
    "    \n",
    "    batch_data_parallel(data_path, chunk_size, used_dims, pidxs, column_names, hdfs_path, lock_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current chunk:  0\n",
      "start thread:  0 thread_0_0\n",
      "current chunk:  1\n",
      "start thread:  1 thread_1_1\n",
      "current chunk:  2\n",
      "start thread:  2 thread_2_2\n",
      "current chunk:  3\n",
      "start thread:  3 thread_3_3\n",
      "current chunk:  4\n",
      "start thread:  4 thread_4_4\n",
      "current chunk:  5\n",
      "start thread:  5 thread_5_5\n",
      "current chunk:  6\n",
      "start thread:  6 thread_6_6\n",
      "current chunk:  7\n",
      "start thread:  7 thread_7_7\n",
      "exit thread:  7 thread_7_7\n",
      "exit thread:  2 thread_2_2\n",
      "exit thread:  4 thread_4_4\n",
      "exit thread:  6 thread_6_6\n",
      "exit thread:  0 thread_0_0\n",
      "exit thread:  5 thread_5_5\n",
      "exit thread:  1 thread_1_1\n",
      "exit thread:  3 thread_3_3\n",
      "===================================================\n",
      "current chunk:  8\n",
      "start thread:  0 thread_0_8\n",
      "current chunk:  9\n",
      "start thread:  1 thread_1_9\n",
      "current chunk:  10\n",
      "start thread:  2 thread_2_10\n",
      "current chunk:  11\n",
      "start thread:  3 thread_3_11\n",
      "current chunk:  12\n",
      "start thread:  4 thread_4_12\n",
      "current chunk:  13\n",
      "start thread:  5 thread_5_13\n",
      "current chunk:  14\n",
      "start thread:  6 thread_6_14\n",
      "current chunk:  15\n",
      "start thread:  7 thread_7_15\n",
      "exit thread:  1 thread_1_9\n",
      "exit thread:  4 thread_4_12\n",
      "exit thread:  2 thread_2_10\n",
      "exit thread:  6 thread_6_14\n",
      "exit thread:  0 thread_0_8\n",
      "exit thread:  5 thread_5_13\n",
      "exit thread:  3 thread_3_11\n",
      "exit thread:  7 thread_7_15\n",
      "===================================================\n",
      "current chunk:  16\n",
      "start thread:  0 thread_0_16\n",
      "current chunk:  17\n",
      "start thread:  1 thread_1_17\n",
      "current chunk:  18\n",
      "start thread:  2 thread_2_18\n",
      "current chunk:  19\n",
      "start thread:  3 thread_3_19\n",
      "current chunk:  20\n",
      "start thread:  4 thread_4_20\n",
      "current chunk:  21\n",
      "start thread:  5 thread_5_21\n",
      "current chunk:  22\n",
      "start thread:  6 thread_6_22\n",
      "current chunk:  23\n",
      "start thread:  7 thread_7_23\n",
      "exit thread:  6 thread_6_22\n",
      "exit thread:  1 thread_1_17\n",
      "exit thread:  5 thread_5_21\n",
      "exit thread:  7 thread_7_23\n",
      "exit thread:  3 thread_3_19\n",
      "exit thread:  2 thread_2_18\n",
      "exit thread:  0 thread_0_16\n",
      "exit thread:  4 thread_4_20\n",
      "===================================================\n",
      "current chunk:  24\n",
      "start thread:  0 thread_0_24\n",
      "current chunk:  25\n",
      "start thread:  1 thread_1_25\n",
      "current chunk:  26\n",
      "start thread:  2 thread_2_26\n",
      "current chunk:  27\n",
      "start thread:  3 thread_3_27\n",
      "current chunk:  28\n",
      "start thread:  4 thread_4_28\n",
      "current chunk:  29\n",
      "start thread:  5 thread_5_29\n",
      "current chunk:  30\n",
      "start thread:  6 thread_6_30\n",
      "current chunk:  31\n",
      "start thread:  7 thread_7_31\n",
      "exit thread:  0 thread_0_24\n",
      "exit thread:  2 thread_2_26\n",
      "exit thread:  6 thread_6_30\n",
      "exit thread:  5 thread_5_29\n",
      "exit thread:  4 thread_4_28\n",
      "exit thread:  3 thread_3_27\n",
      "exit thread:  1 thread_1_25\n",
      "exit thread:  7 thread_7_31\n",
      "===================================================\n",
      "current chunk:  32\n",
      "start thread:  0 thread_0_32\n",
      "current chunk:  33\n",
      "start thread:  1 thread_1_33\n",
      "current chunk:  34\n",
      "start thread:  2 thread_2_34\n",
      "current chunk:  35\n",
      "start thread:  3 thread_3_35\n",
      "current chunk:  36\n",
      "start thread:  4 thread_4_36\n",
      "current chunk:  37\n",
      "start thread:  5 thread_5_37\n",
      "current chunk:  38\n",
      "start thread:  6 thread_6_38\n",
      "current chunk:  39\n",
      "start thread:  7 thread_7_39\n",
      "exit thread:  6 thread_6_38\n",
      "exit thread:  0 thread_0_32\n",
      "exit thread:  1 thread_1_33\n",
      "exit thread:  4 thread_4_36\n",
      "exit thread:  3 thread_3_35\n",
      "exit thread:  5 thread_5_37\n",
      "exit thread:  2 thread_2_34\n",
      "exit thread:  7 thread_7_39\n",
      "===================================================\n",
      "current chunk:  40\n",
      "start thread:  0 thread_0_40\n",
      "current chunk:  41\n",
      "start thread:  1 thread_1_41\n",
      "current chunk:  42\n",
      "start thread:  2 thread_2_42\n",
      "current chunk:  43\n",
      "start thread:  3 thread_3_43\n",
      "current chunk:  44\n",
      "start thread:  4 thread_4_44\n",
      "current chunk:  45\n",
      "start thread:  5 thread_5_45\n",
      "current chunk:  46\n",
      "start thread:  6 thread_6_46\n",
      "current chunk:  47\n",
      "start thread:  7 thread_7_47\n",
      "exit thread:  4 thread_4_44\n",
      "exit thread:  0 thread_0_40\n",
      "exit thread:  2 thread_2_42\n",
      "exit thread:  1 thread_1_41\n",
      "exit thread:  3 thread_3_43\n",
      "exit thread:  7 thread_7_47\n",
      "exit thread:  6 thread_6_46\n",
      "exit thread:  5 thread_5_45\n",
      "===================================================\n",
      "current chunk:  48\n",
      "start thread:  0 thread_0_48\n",
      "current chunk:  49\n",
      "start thread:  1 thread_1_49\n",
      "current chunk:  50\n",
      "start thread:  2 thread_2_50\n",
      "current chunk:  51\n",
      "start thread:  3 thread_3_51\n",
      "current chunk:  52\n",
      "start thread:  4 thread_4_52\n",
      "current chunk:  53\n",
      "start thread:  5 thread_5_53\n",
      "current chunk:  54\n",
      "start thread:  6 thread_6_54\n",
      "current chunk:  55\n",
      "start thread:  7 thread_7_55\n",
      "exit thread:  2 thread_2_50\n",
      "exit thread:  0 thread_0_48\n",
      "exit thread:  1 thread_1_49\n",
      "exit thread:  3 thread_3_51\n",
      "exit thread:  4 thread_4_52\n",
      "exit thread:  6 thread_6_54\n",
      "exit thread:  5 thread_5_53\n",
      "exit thread:  7 thread_7_55\n",
      "===================================================\n",
      "current chunk:  56\n",
      "start thread:  0 thread_0_56\n",
      "current chunk:  57\n",
      "start thread:  1 thread_1_57\n",
      "current chunk:  58\n",
      "start thread:  2 thread_2_58\n",
      "current chunk:  59\n",
      "start thread:  3 thread_3_59\n",
      "current chunk:  60\n",
      "start thread:  4 thread_4_60\n",
      "current chunk:  61\n",
      "start thread:  5 thread_5_61\n",
      "current chunk:  62\n",
      "start thread:  6 thread_6_62\n",
      "current chunk:  63\n",
      "start thread:  7 thread_7_63\n",
      "exit thread:  1 thread_1_57\n",
      "exit thread:  4 thread_4_60\n",
      "exit thread:  6 thread_6_62\n",
      "exit thread:  5 thread_5_61\n",
      "exit thread:  0 thread_0_56\n",
      "exit thread:  7 thread_7_63\n",
      "exit thread:  3 thread_3_59\n",
      "exit thread:  2 thread_2_58\n",
      "===================================================\n",
      "current chunk:  64\n",
      "start thread:  0 thread_0_64\n",
      "current chunk:  65\n",
      "start thread:  1 thread_1_65\n",
      "current chunk:  66\n",
      "start thread:  2 thread_2_66\n",
      "current chunk:  67\n",
      "start thread:  3 thread_3_67\n",
      "current chunk:  68\n",
      "start thread:  4 thread_4_68\n",
      "current chunk:  69\n",
      "start thread:  5 thread_5_69\n",
      "current chunk:  70\n",
      "start thread:  6 thread_6_70\n",
      "current chunk:  71\n",
      "start thread:  7 thread_7_71\n",
      "exit thread:  1 thread_1_65\n",
      "exit thread:  5 thread_5_69\n",
      "exit thread:  7 thread_7_71\n",
      "exit thread:  3 thread_3_67\n",
      "exit thread:  4 thread_4_68\n",
      "exit thread:  0 thread_0_64\n",
      "exit thread:  2 thread_2_66\n",
      "exit thread:  6 thread_6_70\n",
      "===================================================\n",
      "current chunk:  72\n",
      "start thread:  0 thread_0_72\n",
      "current chunk:  73\n",
      "start thread:  1 thread_1_73\n",
      "current chunk:  74\n",
      "start thread:  2 thread_2_74\n",
      "current chunk:  75\n",
      "start thread:  3 thread_3_75\n",
      "current chunk:  76\n",
      "start thread:  4 thread_4_76\n",
      "current chunk:  77\n",
      "start thread:  5 thread_5_77\n",
      "current chunk:  78\n",
      "start thread:  6 thread_6_78\n",
      "current chunk:  79\n",
      "start thread:  7 thread_7_79\n",
      "exit thread:  2 thread_2_74\n",
      "exit thread:  1 thread_1_73\n",
      "exit thread:  6 thread_6_78\n",
      "exit thread:  5 thread_5_77\n",
      "exit thread:  3 thread_3_75\n",
      "exit thread:  0 thread_0_72\n",
      "exit thread:  4 thread_4_76\n",
      "exit thread:  7 thread_7_79\n",
      "===================================================\n",
      "current chunk:  80\n",
      "start thread:  0 thread_0_80\n",
      "current chunk:  81\n",
      "start thread:  1 thread_1_81\n",
      "current chunk:  82\n",
      "start thread:  2 thread_2_82\n",
      "current chunk:  83\n",
      "start thread:  3 thread_3_83\n",
      "current chunk:  84\n",
      "start thread:  4 thread_4_84\n",
      "current chunk:  85\n",
      "start thread:  5 thread_5_85\n",
      "current chunk:  86\n",
      "start thread:  6 thread_6_86\n",
      "current chunk:  87\n",
      "start thread:  7 thread_7_87\n",
      "exit thread:  7 thread_7_87\n",
      "exit thread:  0 thread_0_80\n",
      "exit thread:  3 thread_3_83\n",
      "exit thread:  1 thread_1_81\n",
      "exit thread:  2 thread_2_82\n",
      "exit thread:  6 thread_6_86\n",
      "exit thread:  4 thread_4_84\n",
      "exit thread:  5 thread_5_85\n",
      "===================================================\n",
      "current chunk:  88\n",
      "start thread:  0 thread_0_88\n",
      "current chunk:  89\n",
      "start thread:  1 thread_1_89\n",
      "current chunk:  90\n",
      "start thread:  2 thread_2_90\n",
      "current chunk:  91\n",
      "start thread:  3 thread_3_91\n",
      "current chunk:  92\n",
      "start thread:  4 thread_4_92\n",
      "current chunk:  93\n",
      "start thread:  5 thread_5_93\n",
      "current chunk:  94\n",
      "start thread:  6 thread_6_94\n",
      "current chunk:  95\n",
      "start thread:  7 thread_7_95\n",
      "exit thread:  6 thread_6_94\n",
      "exit thread:  3 thread_3_91\n",
      "exit thread:  0 thread_0_88\n",
      "exit thread:  1 thread_1_89\n",
      "exit thread:  7 thread_7_95\n",
      "exit thread:  2 thread_2_90\n",
      "exit thread:  5 thread_5_93\n",
      "exit thread:  4 thread_4_92\n",
      "===================================================\n",
      "current chunk:  96\n",
      "start thread:  0 thread_0_96\n",
      "current chunk:  97\n",
      "start thread:  1 thread_1_97\n",
      "current chunk:  98\n",
      "start thread:  2 thread_2_98\n",
      "current chunk:  99\n",
      "start thread:  3 thread_3_99\n",
      "current chunk:  100\n",
      "start thread:  4 thread_4_100\n",
      "current chunk:  101\n",
      "start thread:  5 thread_5_101\n",
      "current chunk:  102\n",
      "start thread:  6 thread_6_102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current chunk:  103\n",
      "start thread:  7 thread_7_103\n",
      "exit thread:  2 thread_2_98\n",
      "exit thread:  4 thread_4_100\n",
      "exit thread:  6 thread_6_102\n",
      "exit thread:  1 thread_1_97\n",
      "exit thread:  7 thread_7_103\n",
      "exit thread:  0 thread_0_96\n",
      "exit thread:  3 thread_3_99\n",
      "exit thread:  5 thread_5_101\n",
      "===================================================\n",
      "current chunk:  104\n",
      "start thread:  0 thread_0_104\n",
      "current chunk:  105\n",
      "start thread:  1 thread_1_105\n",
      "current chunk:  106\n",
      "start thread:  2 thread_2_106\n",
      "current chunk:  107\n",
      "start thread:  3 thread_3_107\n",
      "current chunk:  108\n",
      "start thread:  4 thread_4_108\n",
      "current chunk:  109\n",
      "start thread:  5 thread_5_109\n",
      "current chunk:  110\n",
      "start thread:  6 thread_6_110\n",
      "current chunk:  111\n",
      "start thread:  7 thread_7_111\n",
      "exit thread:  0 thread_0_104\n",
      "exit thread:  3 thread_3_107\n",
      "exit thread:  2 thread_2_106\n",
      "exit thread:  6 thread_6_110\n",
      "exit thread:  5 thread_5_109\n",
      "exit thread:  1 thread_1_105\n",
      "exit thread:  7 thread_7_111\n",
      "exit thread:  4 thread_4_108\n",
      "===================================================\n",
      "current chunk:  112\n",
      "start thread:  0 thread_0_112\n",
      "current chunk:  113\n",
      "start thread:  1 thread_1_113\n",
      "current chunk:  114\n",
      "start thread:  2 thread_2_114\n",
      "current chunk:  115\n",
      "start thread:  3 thread_3_115\n",
      "current chunk:  116\n",
      "start thread:  4 thread_4_116\n",
      "current chunk:  117\n",
      "start thread:  5 thread_5_117\n",
      "current chunk:  118\n",
      "start thread:  6 thread_6_118\n",
      "current chunk:  119\n",
      "start thread:  7 thread_7_119\n",
      "exit thread:  4 thread_4_116\n",
      "exit thread:  5 thread_5_117\n",
      "exit thread:  0 thread_0_112\n",
      "exit thread:  7 thread_7_119\n",
      "exit thread:  3 thread_3_115\n",
      "exit thread:  1 thread_1_113\n",
      "exit thread:  6 thread_6_118\n",
      "exit thread:  2 thread_2_114\n",
      "===================================================\n",
      "total data routing and persisting time:  48390.98779940605\n"
     ]
    }
   ],
   "source": [
    "# = = = Execution = = =\n",
    "data_path = '/home/cloudray/Downloads/TPCH_12M_8Field.csv'\n",
    "partition_path = '/home/cloudray/NORA_Partitions/nora_partitions'\n",
    "num_threads = 8\n",
    "chunk_size = 100000\n",
    "used_dims = [1,2]\n",
    "column_names = ['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7']\n",
    "hdfs_path = 'hdfs://localhost:9000/user/cloudray/NORA/'\n",
    "\n",
    "data_routing(data_path, partition_path, num_threads, chunk_size, used_dims, column_names, hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = Unit Test = = =\n",
    "# # path = '/home/cloudray/NORA_Partitions/nora_partitions'\n",
    "# path = '/home/cloudray/NORA_Partitions/qd_tree_partitions'\n",
    "\n",
    "# partitions = load_partitions_from_file(path)\n",
    "\n",
    "# for i in range(len(partitions)):\n",
    "#     if partitions[i][0][0][0] > partitions[i][0][0][1] or partitions[i][0][1][0] > partitions[i][0][1][1]:\n",
    "#         print('found invalid position: ',i, partitions[i])\n",
    "\n",
    "# p = index.Property()\n",
    "# p.leaf_capacity = 100 # cannot be less than 100, indicate the maximum capacity\n",
    "# p.fill_factor = 0.5\n",
    "# p.overwrite = True\n",
    "\n",
    "# # partition_index = index.Index(properties = p)\n",
    "# lock_dict = {}\n",
    "\n",
    "# # the rtree index has some problem in multi-threading.\n",
    "# # that's why we need to create multiple index here, each for 1 thread\n",
    "# pidxs = []\n",
    "# for k in range(8):\n",
    "#     partition_index = index.Index(properties = p)\n",
    "#     for i in range(len(partitions)):\n",
    "#         #partition_index.insert(int(partitions[i][-4]), kdnode_2_border(partitions[i])) # qd-tree do not have this\n",
    "#         partition_index.insert(i, kdnode_2_border(partitions[i]))\n",
    "#         lock_dict.update({i:threading.Lock()})\n",
    "#     pidxs.append(partition_index)\n",
    "    \n",
    "# raw_data_path = '/home/cloudray/Downloads/TPCH_12M_8Field.csv'\n",
    "# chunk_size = 10000\n",
    "# column_names = ['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7'] # handle this\n",
    "# hdfs_path = 'hdfs://localhost:9000/user/cloudray/NORA/'\n",
    "# partition_and_query_dims = [1,2]\n",
    "\n",
    "# # batch_data(raw_data_path, chunk_size, partition_and_query_dims, partition_index, column_names, hdfs_path)\n",
    "# print(partition_index.get_size())\n",
    "\n",
    "# raw_data_path = '/home/cloudray/Downloads/TPCH_12M_8Field.csv'\n",
    "# chunk_size = 10000\n",
    "# column_names = ['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7'] # handle this\n",
    "# hdfs_path = 'hdfs://localhost:9000/user/cloudray/NORA/'\n",
    "# partition_and_query_dims = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = Unit Test = = =\n",
    "# # batch_data_parallel(raw_data_path, chunk_size, partition_and_query_dims, \n",
    "# #                     partition_index, column_names, hdfs_path, lock_dict)\n",
    "\n",
    "# batch_data_parallel(raw_data_path, chunk_size, partition_and_query_dims, \n",
    "#                     pidxs, column_names, hdfs_path, lock_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # = = = check correctness = = =\n",
    "# df = sqlContext.read.parquet('hdfs://localhost:9000/user/cloudray/NORA/partition_19.parquet')\n",
    "# print(df)\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # = = = check correctness = = =\n",
    "# # check correctness for Rtree index, where the pos is reported no partition overlap\n",
    "# overlap_pids = list(partition_index.intersection((118449.0, 8460.0, 118449.0, 8460.0)))\n",
    "# print(overlap_pids)\n",
    "# print(partition_index.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
