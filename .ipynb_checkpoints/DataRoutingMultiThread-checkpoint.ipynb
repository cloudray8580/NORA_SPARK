{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis one is abandoned!\n",
    "# Python multi thread is slower than single thread for CPU-intensive tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import rtree\n",
    "from rtree import index\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk_row(row, used_dims, pidx, pid_data_dict):\n",
    "    row_numpy = row.to_numpy()\n",
    "    row_used_dims_list = row_numpy[used_dims].tolist()\n",
    "    row_border = tuple(row_used_dims_list+row_used_dims_list)\n",
    "    pid = list(pidx.intersection(row_border))[0]\n",
    "    pid_data_dict[pid].append(row_numpy.tolist())\n",
    "\n",
    "\n",
    "class DRThread(threading.Thread):\n",
    "    def __init__(self, thread_id, name, parameters):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.thread_id = thread_id\n",
    "        self.name = name\n",
    "        self.parameters = parameters\n",
    "        \n",
    "    def run(self):\n",
    "        print('start thread: ', self.thread_id, self.name)\n",
    "        chunk, used_dims, pidx, pid_data_dict = self.parameters\n",
    "        chunk.apply(lambda row: process_chunk_row(row, used_dims, pidx, pid_data_dict), axis=1)\n",
    "        print('exit thread: ', self.thread_id, self.name)\n",
    "        \n",
    "class DumpThread(threading.Thread):\n",
    "    def __init__(self, thread_id, name, parameters):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.thread_id = thread_id\n",
    "        self.name = name\n",
    "        self.parameters = parameters\n",
    "        \n",
    "    def run(self):\n",
    "        print('start dumping thread: ', self.thread_id, self.name)\n",
    "        start_index, end_index, pids, pid_data_dict, hdfs_path, column_names = self.parameters\n",
    "        for pid in pids[start_index: end_index]:\n",
    "            path = hdfs_path + 'partition_' + str(pid)+'.parquet'\n",
    "            pdf = pd.DataFrame(pid_data_dict[pid], columns=column_names)\n",
    "            df = sqlContext.createDataFrame(pdf)\n",
    "            df.write.mode('append').parquet(path)\n",
    "            pid_data_dict[pid] = []\n",
    "        print('exit dumping thread: ', self.thread_id, self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kdnode_2_border(kdnode):\n",
    "    lower = [domain[0] for domain in kdnode[0]]\n",
    "    upper = [domain[1] for domain in kdnode[0]]\n",
    "    border = tuple(lower + upper) # non interleave\n",
    "    return border\n",
    "\n",
    "def load_partitions_from_file(path):\n",
    "    '''\n",
    "    the loaded stretched_kdnodes: [num_dims, l1,l2,...,ln, u1,u2,...,un, size, id, pid, left_child,id, right_child_id]\n",
    "    '''\n",
    "    stretched_kdnodes = genfromtxt(path, delimiter=',')\n",
    "    num_dims = int(stretched_kdnodes[0,0])\n",
    "    kdnodes = []\n",
    "    for i in range(len(stretched_kdnodes)):\n",
    "        domains = [ [stretched_kdnodes[i,k+1],stretched_kdnodes[i,1+num_dims+k]] for k in range(num_dims) ]\n",
    "        row = [domains]\n",
    "        row.append(stretched_kdnodes[i,2*num_dims+1])\n",
    "        # to be compatible with qd-tree's partition, that do not have the last 4 attributes\n",
    "        if len(stretched_kdnodes[i]) > 2*num_dims+2:\n",
    "            row.append(stretched_kdnodes[i,-4])\n",
    "            row.append(stretched_kdnodes[i,-3])\n",
    "            row.append(stretched_kdnodes[i,-2])\n",
    "            row.append(stretched_kdnodes[i,-1])\n",
    "        kdnodes.append(row)\n",
    "    return kdnodes\n",
    "\n",
    "# def dump_data_thread(start_index, end_index, pids, pid_data_dict, hdfs_path):\n",
    "#     for pid in pids[start_index, end_index]:\n",
    "#         path = hdfs_path + 'partition_' + str(pid)+'.parquet'\n",
    "#         pdf = pd.DataFrame(pid_data_dict[pid], columns=column_names)\n",
    "#         df = sqlContext.createDataFrame(pdf)\n",
    "#         df.write.mode('append').parquet(path)\n",
    "\n",
    "def dump_dict_data_2_hdfs(pid_data_dict, column_names, hdfs_path, num_threads = 8):\n",
    "    pids = list(pid_data_dict.keys())\n",
    "    step = int(len(pids) / num_threads) + 1\n",
    "    threads = []\n",
    "    for i in range(num_threads):\n",
    "        start_index = i * step\n",
    "        end_index = (i+1) * step\n",
    "        parameters = [start_index, end_index, pids, pid_data_dict, hdfs_path, column_names]\n",
    "        thread = DumpThread(i, 'dump_thread_'+str(i), parameters)\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "        if start_index >= len(pids):\n",
    "            break   \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "\n",
    "def batch_data_parallel(table_path, partition_path, chunk_size, used_dims, hdfs_path, \n",
    "                        num_dims, dump_threshold = 1000000, max_threads = 8):\n",
    "    \n",
    "    begin_time = time.time()\n",
    "    \n",
    "    col_names = ['_c'+str(i) for i in range(num_dims)]\n",
    "    cols = [i for i in range(num_dims)]\n",
    "    \n",
    "    partitions = load_partitions_from_file(partition_path)\n",
    "    \n",
    "    p = index.Property()\n",
    "    p.leaf_capacity = 32\n",
    "    p.index_capacity =32\n",
    "    p.NearMinimumOverlaoFactor = 16\n",
    "    p.fill_factor = 0.8\n",
    "    p.overwrite = True\n",
    "    \n",
    "    pidxs = [] # the rtree index has problem in mutli-threading, create an index for each thread\n",
    "    for k in range(num_threads):\n",
    "        partition_index = index.Index(properties = p)\n",
    "        for i in range(len(partitions)):\n",
    "            #partition_index.insert(int(partitions[i][-4]), kdnode_2_border(partitions[i])) \n",
    "            partition_index.insert(i, kdnode_2_border(partitions[i]))\n",
    "        pidxs.append(partition_index)\n",
    "    \n",
    "    pid_data_dict = {}\n",
    "    for i in range(len(partitions)):\n",
    "        pid_data_dict.update({i:[]})\n",
    "    \n",
    "    count = 0\n",
    "    epochs = 0\n",
    "    processed_data = 0\n",
    "    threads = []\n",
    "    #for chunk in pd.read_table(table_path, delimiter='|', usecols=cols, names=col_names, chunksize=chunk_size):\n",
    "    for chunk in pd.read_csv(table_path, usecols=cols, names=col_names, chunksize=chunk_size):\n",
    "        \n",
    "        print('current chunk: ', count)\n",
    "        tid = count % max_threads      \n",
    "        parameters = [chunk, used_dims, pidxs[tid], pid_data_dict]\n",
    "        thread = DRThread(tid, 'thread_'+str(tid)+'_'+str(count), parameters)\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "        count += 1\n",
    "        \n",
    "        if tid == max_threads-1:\n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            threads = []\n",
    "            epochs += 1\n",
    "            processed_data += chunk_size * max_threads\n",
    "            if processed_data >= dump_threshold:\n",
    "                dump_dict_data_2_hdfs(pid_data_dict, col_names, hdfs_path)\n",
    "                for key in pid_data_dict.keys():\n",
    "                    pid_data_dict[key]=[]\n",
    "                processed_data = 0\n",
    "                \n",
    "            print('===================================================')\n",
    "    dump_dict_data_2_hdfs(pid_data_dict, col_names, hdfs_path) # last batch\n",
    "    \n",
    "    finish_time = time.time()\n",
    "    print('total data routing and persisting time: ', finish_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = Execution = = =\n",
    "table_path = '/home/cloudray/Downloads/TPCH_12M_8Field.csv'\n",
    "#table_path = '/home/cloudray/TPCH/2.18.0_rc2/dbgen/lineitem.tbl'\n",
    "\n",
    "partition_path = '/home/cloudray/NORA_Partitions/nora_partitions'\n",
    "# partition_path = '/home/cloudray/NORA_Partitions/qd_tree_partitions'\n",
    "num_threads = 8\n",
    "num_dims = 8\n",
    "chunk_size = 100000\n",
    "used_dims = [1,2]\n",
    "hdfs_path = 'hdfs://localhost:9000/user/cloudray/NORA/'\n",
    "# hdfs_path = 'hdfs://localhost:9000/user/cloudray/QdTree/'\n",
    "\n",
    "\n",
    "batch_data_parallel(table_path, partition_path, chunk_size, used_dims, hdfs_path,\n",
    "                    num_dims, dump_threshold = 10000000, max_threads = num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try concurrent write to hdfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
