{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import rtree\n",
    "from rtree import index\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import threading\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([(\"spark.executor.memory\", \"8g\"),(\"spark.driver.memory\",\"8g\"),\n",
    "                           (\"spark.memory.offHeap.enabled\",True),(\"spark.memory.offHeap.size\",\"8g\")])\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kdnode_2_border(kdnode):\n",
    "    lower = [domain[0] for domain in kdnode[0]]\n",
    "    upper = [domain[1] for domain in kdnode[0]]\n",
    "    border = tuple(lower + upper) # non interleave\n",
    "    return border\n",
    "\n",
    "def load_partitions_from_file(path):\n",
    "    stretched_kdnodes = genfromtxt(path, delimiter=',')\n",
    "    num_dims = int(stretched_kdnodes[0,0])\n",
    "    kdnodes = []\n",
    "    for i in range(len(stretched_kdnodes)):\n",
    "        domains = [ [stretched_kdnodes[i,k+1],stretched_kdnodes[i,1+num_dims+k]] for k in range(num_dims) ]\n",
    "        row = [domains]\n",
    "        row.append(stretched_kdnodes[i,2*num_dims+1])\n",
    "        # to be compatible with qd-tree's partition, that do not have the last 4 attributes\n",
    "        if len(stretched_kdnodes[i]) > 2*num_dims+2:\n",
    "            row.append(stretched_kdnodes[i,-4])\n",
    "            row.append(stretched_kdnodes[i,-3])\n",
    "            row.append(stretched_kdnodes[i,-2])\n",
    "            row.append(stretched_kdnodes[i,-1])\n",
    "        kdnodes.append(row)\n",
    "    return kdnodes\n",
    "\n",
    "def process_chunk_row(row, used_dims, pidx, pid_data_dict, count, k):\n",
    "    if count[0] % 100000 == 0:\n",
    "        print('proces',k,'has routed',count[0],'rows')\n",
    "    count[0] += 1\n",
    "    row_numpy = row.to_numpy()\n",
    "    row_used_dims_list = row_numpy[used_dims].tolist()\n",
    "    row_border = tuple(row_used_dims_list+row_used_dims_list)\n",
    "    try:\n",
    "        pid = list(pidx.intersection(row_border))[0]\n",
    "    except:\n",
    "        print(row_border)\n",
    "    if pid in pid_data_dict:\n",
    "        pid_data_dict[pid]+=[row_numpy.tolist()]\n",
    "        #print('update dict..')\n",
    "    else:\n",
    "        pid_data_dict[pid]=[row_numpy.tolist()]\n",
    "        #print('initialize dict..')\n",
    "\n",
    "@ray.remote\n",
    "def process_chunk(chunk, used_dims, partition_path, k):\n",
    "    print(\"enter data routing process\", k, '..')    \n",
    "    pid_data_dict = {}\n",
    "    partitions = load_partitions_from_file(partition_path)\n",
    "    p = index.Property()\n",
    "    p.leaf_capacity = 32\n",
    "    p.index_capacity = 32\n",
    "    p.NearMinimumOverlaoFactor = 16\n",
    "    p.fill_factor = 0.8\n",
    "    p.overwrite = True\n",
    "    pidx = index.Index(properties = p)\n",
    "    for i in range(len(partitions)):\n",
    "        pidx.insert(i, kdnode_2_border(partitions[i]))\n",
    "    count = [0]\n",
    "    chunk.apply(lambda row: process_chunk_row(row, used_dims, pidx, pid_data_dict, count, k), axis=1)\n",
    "    dict_id = ray.put(pid_data_dict)\n",
    "    print(\"exit data routing process\", k, \".\")\n",
    "    return dict_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(base_dict, new_dict):\n",
    "    for key, val in new_dict.items():\n",
    "        if key in base_dict:\n",
    "            base_dict[key] += val\n",
    "        else:\n",
    "            base_dict[key] = val\n",
    "    new_dict.clear()\n",
    "\n",
    "class DumpThread(threading.Thread):\n",
    "    def __init__(self, thread_id, parameters):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.thread_id = thread_id\n",
    "        self.parameters = parameters\n",
    "        \n",
    "    def run(self):\n",
    "        print('= = = start dumping thread = = =')\n",
    "        start_index, end_index, pids, merged_dict, pq_writers, fs, hdfs_path, column_names = self.parameters\n",
    "        for pid in pids[start_index: end_index]:\n",
    "            path = hdfs_path + 'partition_' + str(pid)+'.parquet'\n",
    "            pdf = pd.DataFrame(merged_dict[pid], columns=column_names)\n",
    "            adf = pa.Table.from_pandas(pdf)\n",
    "            if pid in pq_writers:\n",
    "                pq_writers[pid].write_table(table=adf)\n",
    "                #print('existing pid')\n",
    "            else:\n",
    "                writer = pq.ParquetWriter(path, adf.schema, fs)\n",
    "                pq_writers[pid] = writer\n",
    "                writer.write_table(table=adf)\n",
    "                #print('new pid', pid, 'from thread',self.thread_id)\n",
    "            merged_dict[pid] = []\n",
    "        print('= = = exit dumping thread = = =')\n",
    "\n",
    "@ray.remote\n",
    "def dump_dict_2_hdfs(merged_dict, pq_writers, fs, column_names, hdfs_path, num_threads):\n",
    "                                         \n",
    "    if num_threads == 1:\n",
    "        print('start dumping with single thread in main process..')\n",
    "        pids = list(merged_dict.keys())\n",
    "        for pid in pids:\n",
    "            #print(\"writing to pid:\",pid)\n",
    "            path = hdfs_path + 'partition_' + str(pid)+'.parquet'\n",
    "            pdf = pd.DataFrame(merged_dict[pid], columns=column_names)\n",
    "            adf = pa.Table.from_pandas(pdf)\n",
    "            if pid in pq_writers:\n",
    "                pq_writers[pid].write_table(table=adf)\n",
    "                #print('existing pid')\n",
    "            else:\n",
    "                writer = pq.ParquetWriter(path, adf.schema, fs)\n",
    "                print(\"adf.schema:\",adf.schema)\n",
    "                pq_writers[pid] = writer\n",
    "                writer.write_table(table=adf)\n",
    "                #print('new pid')\n",
    "            merged_dict[pid] = []  \n",
    "    else:\n",
    "        print('start dumping with', num_threads, 'threads in main process..')\n",
    "        pids = list(merged_dict.keys())\n",
    "        step = int(len(pids) / num_threads) + 1\n",
    "        threads = []\n",
    "        for i in range(num_threads):\n",
    "            start_index = i * step\n",
    "            end_index = (i+1) * step\n",
    "            parameters = [start_index, end_index, pids, merged_dict, pq_writers, fs, hdfs_path, column_names]\n",
    "            thread = DumpThread(i, parameters)\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "            if start_index >= len(pids):\n",
    "                break   \n",
    "        for t in threads:\n",
    "            t.join()\n",
    "    print('finish dumping.')\n",
    "\n",
    "\n",
    "def dump_dict_2_hdfs_simple(merged_dict, pq_writers, column_names, hdfs_path, fs):\n",
    "    print('= = = start dumping in main thread = = =')\n",
    "    for pid, val in merged_dict.items():\n",
    "        #print(\"writing to pid:\",pid)\n",
    "        path = hdfs_path + 'partition_' + str(pid)+'.parquet'\n",
    "        pdf = pd.DataFrame(val, columns=column_names)\n",
    "        adf = pa.Table.from_pandas(pdf)\n",
    "        if pid in pq_writers:\n",
    "            pq_writers[pid].write_table(table=adf)\n",
    "        else:\n",
    "            writer = pq.ParquetWriter(path, adf.schema, fs)\n",
    "            pq_writers[pid] = writer\n",
    "            writer.write_table(table=adf)\n",
    "    print('= = = exit dumping = = =')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_writers(table_path, cols, col_names, partition_path, hdfs_path, fs):\n",
    "    \n",
    "    schema = None\n",
    "    for chunk in pd.read_table(table_path, delimiter='|', usecols=cols, names=col_names, chunksize=10):\n",
    "        schema = pa.Schema.from_pandas(chunk)\n",
    "        break\n",
    "    \n",
    "    stretched_kdnodes = genfromtxt(partition_path, delimiter=',')\n",
    "    num_nodes = len(stretched_kdnodes)\n",
    "    \n",
    "    pq_writers = {}\n",
    "    for i in range(num_nodes):\n",
    "        path = hdfs_path + 'partition_' + str(i)+'.parquet'\n",
    "        pq_writers[i] = pq.ParquetWriter(path, schema, fs)\n",
    "    \n",
    "    return pq_writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_parallel(table_path, partition_path, chunk_size, used_dims, hdfs_path, num_dims, num_process, hdfs_private_ip):\n",
    "    \n",
    "    begin_time = time.time()\n",
    "    \n",
    "    ray.init(num_cpus=num_process)\n",
    "    \n",
    "    # column names for pandas dataframe\n",
    "    cols = [i for i in range(num_dims)]\n",
    "    col_names = ['_c'+str(i) for i in range(num_dims)]\n",
    "    \n",
    "    # pyarrow parquent append\n",
    "    pq_writers = {}\n",
    "    fs = pa.fs.HadoopFileSystem(hdfs_private_ip, port=9000, user='hdfs', replication=1)\n",
    "    \n",
    "    # chunks\n",
    "    chunk_count = 0\n",
    "    \n",
    "    # collect object refs\n",
    "    chunk_ids = []\n",
    "    result_ids = []\n",
    "    \n",
    "    dump_id = None\n",
    "    first_loop = True\n",
    "    \n",
    "    for chunk in pd.read_table(table_path, delimiter='|', usecols=cols, names=col_names, chunksize=chunk_size):\n",
    "        print('reading chunk: ', chunk_count)\n",
    "        \n",
    "        chunk_id = ray.put(chunk)\n",
    "        chunk_ids.append(chunk_id)\n",
    "        del chunk_id\n",
    "        \n",
    "        result_id = process_chunk.remote(chunk_ids[chunk_count % num_process], used_dims, partition_path, chunk_count % num_process)\n",
    "        result_ids.append(result_id)\n",
    "        del result_id\n",
    "        \n",
    "        # after all process allocated a chunk, process and dump the data\n",
    "        if chunk_count % num_process == num_process - 1:\n",
    "            \n",
    "            print(\"= = = = = = Synchronization Barrier = = = = = =\")\n",
    "            base_dict = {}\n",
    "            while len(result_ids):\n",
    "                done_id, result_ids = ray.wait(result_ids)\n",
    "                dict_id = ray.get(done_id[0])\n",
    "                result_dict = ray.get(dict_id)\n",
    "                dump_dict_2_hdfs_simple(result_dict, pq_writers, col_names, hdfs_path, fs)\n",
    "                #merge_dict(base_dict, result_dict)\n",
    "            print(\"= = finish merged = =\")\n",
    "            \n",
    "            chunk_ids.clear() # clear up the references\n",
    "            result_ids.clear() # clear up the references\n",
    "            base_dict.clear()\n",
    "                     \n",
    "            current_time = time.time()\n",
    "            time_elapsed = current_time - begin_time\n",
    "            print(\"= = = TOTAL PROCESSED SO FAR:\", (chunk_count+1) * chunk_size,\"ROWS. TIME SPENT:\", time_elapsed, \"SECONDS = = =\")\n",
    "                \n",
    "        chunk_count += 1\n",
    "        \n",
    "    print('after exit, chunks size: ', len(chunk_ids))\n",
    "    # process the last batch\n",
    "    if len(chunk_ids) != 0:\n",
    "        print(\"= = = = = = Synchronization Barrier = = = = = =\")\n",
    "        base_dict = {}\n",
    "        while len(result_ids):\n",
    "            done_id, result_ids = ray.wait(result_ids)\n",
    "            dict_id = ray.get(done_id[0])\n",
    "            result_dict = ray.get(dict_id)\n",
    "            merge_dict(base_dict, result_dict)\n",
    "        print(\"= = finish merged = =\")\n",
    "        chunk_ids.clear() # clear up the references\n",
    "        result_ids.clear() # clear up the references\n",
    "        dump_dict_2_hdfs_simple(base_dict, pq_writers, col_names, hdfs_path, fs) \n",
    "        base_dict.clear()\n",
    "        \n",
    "    \n",
    "    for pid, writer in pq_writers.items():\n",
    "        writer.close()\n",
    "    \n",
    "    ray.shutdown()\n",
    "    \n",
    "    finish_time = time.time()\n",
    "    print('total data routing and persisting time: ', finish_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = Configuration (UBDA Cloud Centos) = = =\n",
    "scale_factor = 100\n",
    "\n",
    "table_base_path = '/media/datadrive1/TPCH/dbgen/'\n",
    "table_path = table_base_path + 'lineitem_' + str(scale_factor) + '.tbl'\n",
    "\n",
    "num_process = 8\n",
    "chunk_size = 3000000\n",
    "# 6M rows = about 1GB raw data\n",
    "\n",
    "num_dims = 16\n",
    "used_dims = [1,2]\n",
    "\n",
    "# base path of HDFS\n",
    "hdfs_private_ip = '192.168.6.62'\n",
    "hdfs_base_path = 'hdfs://192.168.6.62:9000/user/cloudray/'\n",
    "\n",
    "nora_hdfs = hdfs_base_path + 'NORA/scale' + str(scale_factor) + '/'\n",
    "qdtree_hdfs = hdfs_base_path + 'QdTree/scale' + str(scale_factor) + '/'\n",
    "kdtree_hdfs = hdfs_base_path + 'KDTree/scale' + str(scale_factor) + '/'\n",
    "\n",
    "# base path of Partition\n",
    "partition_base_path = '/home/centos/PartitionLayout/'\n",
    "\n",
    "nora_partition = partition_base_path + 'nora_partitions_' + str(scale_factor)\n",
    "qdtree_partition = partition_base_path + 'qdtree_partitions_' + str(scale_factor)\n",
    "kdtree_partition = partition_base_path + 'kdtree_partitions_' + str(scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-28 23:03:15,094\tINFO services.py:1164 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading chunk:  0\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m enter data routing process 0 ..\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 0 rows\n",
      "reading chunk:  1\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m enter data routing process 1 ..\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 300000 rows\n",
      "reading chunk:  2\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m enter data routing process 2 ..\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 600000 rows\n",
      "reading chunk:  3\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m enter data routing process 3 ..\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 600000 rows\n",
      "reading chunk:  4\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m enter data routing process 4 ..\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 600000 rowsreading chunk:  5\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m enter data routing process 5 ..\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 600000 rowsreading chunk:  6\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m enter data routing process 6 ..\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 600000 rows\n",
      "reading chunk:  7\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 700000 rows\n",
      "= = = = = = Synchronization Barrier = = = = = =\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m enter data routing process 7 ..\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 1200000 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m proces 0 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m proces 1 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27111)\u001b[0m exit data routing process 0 .\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 2100000 rows\n",
      "= = = start dumping in main thread = = =\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m proces 2 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27106)\u001b[0m exit data routing process 1 .\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m proces 3 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 2000000 rows\n",
      "\u001b[2m\u001b[36m(pid=27108)\u001b[0m exit data routing process 2 .\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 2100000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m proces 4 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 2200000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 2500000 rows\n",
      "\u001b[2m\u001b[36m(pid=27105)\u001b[0m exit data routing process 3 .\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 2300000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 2600000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 2400000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m proces 5 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27110)\u001b[0m exit data routing process 4 .\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 2500000 rows\n",
      "= = = exit dumping = = =\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 2600000 rows\n",
      "= = = start dumping in main thread = = =\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 2700000 rows\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m proces 6 has routed 2900000 rows\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 2800000 rows\n",
      "\u001b[2m\u001b[36m(pid=27104)\u001b[0m exit data routing process 5 .\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m proces 7 has routed 2900000 rows\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "\u001b[2m\u001b[36m(pid=27107)\u001b[0m exit data routing process 6 .\n",
      "\u001b[2m\u001b[36m(pid=27109)\u001b[0m exit data routing process 7 .\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "= = = exit dumping = = =\n",
      "= = = start dumping in main thread = = =\n",
      "= = = exit dumping = = =\n",
      "= = finish merged = =\n",
      "finish kdtree data routing..\n"
     ]
    }
   ],
   "source": [
    "# = = = Execution = = =\n",
    "if __name__ == '__main__':\n",
    "    # batch_data_parallel(table_path, nora_partition, chunk_size, used_dims, nora_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "    # print('finish nora data routing..')\n",
    "    # batch_data_parallel(table_path, qdtree_partition, chunk_size, used_dims, qdtree_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "    # print('finish qdtree data routing..')\n",
    "    batch_data_parallel(table_path, kdtree_partition, chunk_size, used_dims, kdtree_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "    print('finish kdtree data routing..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
