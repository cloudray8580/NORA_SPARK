{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL Execution\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query_to_sql(query, used_dims, column_name_dict, hdfs_path):\n",
    "    sql = ''\n",
    "    for i, dim in enumerate(used_dims):\n",
    "        if query[i][0] != -1:\n",
    "            sql += column_name_dict[dim] + '>=' + str(query[i][0]) + ' and '\n",
    "        if query[i][1] != -1:\n",
    "            sql += column_name_dict[dim] + '<=' + str(query[i][1]) + ' and '\n",
    "    sql = sql[0:-4] # remove the last 'and '\n",
    "    \n",
    "    sql = \"SELECT * FROM parquet.`\" + hdfs_path + \"`WHERE \" + sql\n",
    "    \n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_parquets(query, used_dims, column_name_dict, hdfs_path, print_execution_time=False):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sql = transform_query_to_sql(query, used_dims, column_name_dict, hdfs_path)\n",
    "    \n",
    "    end_time_1 = time.time()\n",
    "    \n",
    "    query_result = spark.sql(sql).collect()\n",
    "    \n",
    "    end_time_2 = time.time()\n",
    "    \n",
    "    query_translation_time = end_time_1 - start_time\n",
    "    query_execution_time = end_time_2 - end_time_1\n",
    "    \n",
    "    if print_execution_time:\n",
    "        print('query translation time: ', query_translation_time)\n",
    "        print('query execution time: ', query_execution_time)\n",
    "    \n",
    "    return (query_result, query_translation_time, query_execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_query(path):\n",
    "    query_set = np.genfromtxt(path, delimiter=' ')\n",
    "    query_set = query_set.reshape(len(query_set),-1,2)\n",
    "    return query_set\n",
    "\n",
    "def batch_query(queryset, used_dims, column_name_dict, hdfs_path):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # add statistics result\n",
    "    results = []\n",
    "    for query in queryset:\n",
    "        result = query_with_parquets(query, used_dims, column_name_dict, hdfs_path)\n",
    "        results.append(result)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('total query response time: ', end_time - start_time)\n",
    "    print('average query response time: ', (end_time - start_time) / len(queryset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = Configuration = = =\n",
    "\n",
    "scale_factor = 2\n",
    "query_base_path = '/home/cloudray/NORA_Query/'\n",
    "\n",
    "distribution_path = query_base_path + 'distribution_' + str(scale_factor) + '.csv'\n",
    "random_path = query_base_path + 'random_' + str(scale_factor) + '.csv'\n",
    "\n",
    "distribution_query = load_query(distribution_path)\n",
    "random_query = load_query(random_path)\n",
    "\n",
    "training_set_percentage = 0.5\n",
    "Td = int(len(distribution_query) * training_set_percentage)\n",
    "Tr = int(len(random_query) * training_set_percentage)\n",
    "\n",
    "training_set = np.concatenate((distribution_query[0:Td], random_query[0:Tr]), axis=0)\n",
    "testing_set = np.concatenate((distribution_query[Td:], random_query[Tr:]), axis=0)\n",
    "\n",
    "used_dims = [1,2]\n",
    "num_dims = 16\n",
    "column_names = ['_c'+str(i) for i in range(num_dims)]\n",
    "column_name_dict = {}\n",
    "for i in range(num_dims):\n",
    "    column_name_dict[i] = column_names[i]\n",
    "\n",
    "hdfs_path_nora = 'hdfs://localhost:9000/user/cloudray/NORA/merged/'\n",
    "hdfs_path_qdtree = 'hdfs://localhost:9000/user/cloudray/QdTree/merged/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total query response time:  110.49015402793884\n",
      "average query response time:  2.209803080558777\n"
     ]
    }
   ],
   "source": [
    "# NORA\n",
    "batch_query(testing_set, used_dims, column_name_dict, hdfs_path_nora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total query response time:  19.17433786392212\n",
      "average query response time:  0.3834867572784424\n"
     ]
    }
   ],
   "source": [
    "# Qd-Tree\n",
    "batch_query(testing_set, used_dims, column_name_dict, hdfs_path_qdtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
