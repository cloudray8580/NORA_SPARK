{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Execution\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import rtree\n",
    "from rtree import index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlap_parquets(query, partition_index):\n",
    "    '''\n",
    "    find out all the overlap partition ids\n",
    "    '''\n",
    "    query_lower = [qr[0] for qr in query]\n",
    "    query_upper = [qr[1] for qr in query]\n",
    "    query_border = tuple(query_lower + query_upper)\n",
    "    overlap_pids = list(partition_index.intersection(query_border))\n",
    "    \n",
    "    return overlap_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query_to_sql(query, used_dims, column_name_dict, hdfs_path, pids=None):\n",
    "    sql = ''\n",
    "    for i, dim in enumerate(used_dims):\n",
    "        if query[i][0] != -1:\n",
    "            sql += column_name_dict[dim] + '>=' + str(query[i][0]) + ' and '\n",
    "        if query[i][1] != -1:\n",
    "            sql += column_name_dict[dim] + '<=' + str(query[i][1]) + ' and '\n",
    "    sql = sql[0:-4] # remove the last 'and '\n",
    "    \n",
    "    if pids == None:\n",
    "        #sql = \"SELECT * FROM parquet.`\" + hdfs_path + \"`WHERE \" + sql\n",
    "        sql = \"SELECT variance(_c0) FROM parquet.`\" + hdfs_path + \"`WHERE \" + sql\n",
    "    else:\n",
    "        pids = str(set(pids)).replace(\" \", \"\")\n",
    "        #sql = \"SELECT * FROM parquet.`\" + hdfs_path + 'partition_' + pids + \".parquet` WHERE \" + sql\n",
    "        sql = \"SELECT variance(_c0) FROM parquet.`\" + hdfs_path + 'partition_' + pids + \".parquet` WHERE \" + sql\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_parquets(query, used_dims, column_name_dict, hdfs_path, rtree_idx=None, print_execution_time=False):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sql = None\n",
    "    if rtree_idx == None:\n",
    "        sql = transform_query_to_sql(query, used_dims, column_name_dict, hdfs_path)\n",
    "    else:\n",
    "        pids = find_overlap_parquets(query, rtree_idx)\n",
    "        sql = transform_query_to_sql(query, used_dims, column_name_dict, hdfs_path, pids)\n",
    "    \n",
    "    end_time_1 = time.time()\n",
    "    \n",
    "    query_result = spark.sql(sql).collect()\n",
    "    \n",
    "    end_time_2 = time.time()\n",
    "    \n",
    "    query_translation_time = end_time_1 - start_time\n",
    "    query_execution_time = end_time_2 - end_time_1\n",
    "    \n",
    "    if print_execution_time:\n",
    "        print('query translation time: ', query_translation_time)\n",
    "        print('query execution time: ', query_execution_time)\n",
    "    \n",
    "    #return (query_result, query_translation_time, query_execution_time) # this takes too much memory\n",
    "    return (query_translation_time, query_execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_query(path):\n",
    "    query_set = np.genfromtxt(path, delimiter=' ')\n",
    "    query_set = query_set.reshape(len(query_set),-1,2)\n",
    "    return query_set\n",
    "\n",
    "def batch_query(queryset, used_dims, column_name_dict, hdfs_path, rtree_idx=None):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # add statistics result\n",
    "    results = []\n",
    "    count = 0\n",
    "    for query in queryset:\n",
    "        result = query_with_parquets(query, used_dims, column_name_dict, hdfs_path, rtree_idx)\n",
    "        print('finish query', count)\n",
    "        count += 1\n",
    "        results.append(result)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('total query response time: ', end_time - start_time)\n",
    "    print('average query response time: ', (end_time - start_time) / len(queryset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # = = = Configuration (COMP Cloud Ubuntu) = = =\n",
    "\n",
    "# scale_factor = 100\n",
    "# # query_base_path = '/home/cloudray/NORA_Query/'\n",
    "# query_base_path = '/home/ubuntu/Queryset/'\n",
    "\n",
    "# distribution_path = query_base_path + 'distribution_' + str(scale_factor) + '.csv'\n",
    "# random_path = query_base_path + 'random_' + str(scale_factor) + '.csv'\n",
    "\n",
    "# distribution_query = load_query(distribution_path)\n",
    "# random_query = load_query(random_path)\n",
    "\n",
    "# training_set_percentage = 0.5\n",
    "# Td = int(len(distribution_query) * training_set_percentage)\n",
    "# Tr = int(len(random_query) * training_set_percentage)\n",
    "\n",
    "# training_set = np.concatenate((distribution_query[0:Td], random_query[0:Tr]), axis=0)\n",
    "# testing_set = np.concatenate((distribution_query[Td:], random_query[Tr:]), axis=0)\n",
    "\n",
    "# used_dims = [1,2]\n",
    "# num_dims = 16\n",
    "# column_names = ['_c'+str(i) for i in range(num_dims)]\n",
    "# column_name_dict = {}\n",
    "# for i in range(num_dims):\n",
    "#     column_name_dict[i] = column_names[i]\n",
    "\n",
    "# # hdfs_path_nora = 'hdfs://localhost:9000/user/cloudray/NORA/merged/'\n",
    "# # hdfs_path_qdtree = 'hdfs://localhost:9000/user/cloudray/QdTree/merged/'\n",
    "\n",
    "# hdfs_path_nora = 'hdfs://10.88.88.103:9000/user/cloudray/NORA/scale100/merged/'\n",
    "# hdfs_path_qdtree = 'hdfs://10.88.88.103:9000/user/cloudray/QdTree/scale100/merged/'\n",
    "# hdfs_path_kdtree = 'hdfs://10.88.88.103:9000/user/cloudray/KDTree/scale100/merged/'\n",
    "\n",
    "# # partition_base_path = '/home/ubuntu/PartitionLayout/'\n",
    "# # nora_partition = partition_base_path + 'nora_partitions_' + str(scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = Configuration (UBDA Cloud Centos) = = =\n",
    "\n",
    "scale_factor = 100\n",
    "# query_base_path = '/home/cloudray/NORA_Query/'\n",
    "query_base_path = '/home/centos/Queryset/'\n",
    "\n",
    "distribution_path = query_base_path + 'distribution_' + str(scale_factor) + '.csv'\n",
    "random_path = query_base_path + 'random_' + str(scale_factor) + '.csv'\n",
    "\n",
    "distribution_query = load_query(distribution_path)\n",
    "random_query = load_query(random_path)\n",
    "\n",
    "training_set_percentage = 0.5\n",
    "Td = int(len(distribution_query) * training_set_percentage)\n",
    "Tr = int(len(random_query) * training_set_percentage)\n",
    "\n",
    "training_set = np.concatenate((distribution_query[0:Td], random_query[0:Tr]), axis=0)\n",
    "testing_set = np.concatenate((distribution_query[Td:], random_query[Tr:]), axis=0)\n",
    "\n",
    "used_dims = [1,2]\n",
    "num_dims = 16\n",
    "column_names = ['_c'+str(i) for i in range(num_dims)]\n",
    "column_name_dict = {}\n",
    "for i in range(num_dims):\n",
    "    column_name_dict[i] = column_names[i]\n",
    "\n",
    "# hdfs_path_nora = 'hdfs://localhost:9000/user/cloudray/NORA/merged/'\n",
    "# hdfs_path_qdtree = 'hdfs://localhost:9000/user/cloudray/QdTree/merged/'\n",
    "\n",
    "hdfs_path_nora = 'hdfs://192.168.6.62:9000/user/cloudray/NORA/scale100/merged/'\n",
    "hdfs_path_qdtree = 'hdfs://192.168.6.62:9000/user/cloudray/QdTree/scale100/merged/'\n",
    "hdfs_path_kdtree = 'hdfs://192.168.6.62:9000/user/cloudray/KDTree/scale100/merged/'\n",
    "\n",
    "# partition_base_path = '/home/ubuntu/PartitionLayout/'\n",
    "# nora_partition = partition_base_path + 'nora_partitions_' + str(scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test query\n",
    "# notice, there should not be any white space between and two pids\n",
    "# sql = 'SELECT * FROM parquet.`hdfs://10.88.88.103:9000/user/cloudray/NORA/scale100/merged/partition_{164,165}.parquet`'\n",
    "# sql = 'SELECT variance(_c0) FROM parquet.`hdfs://10.88.88.103:9000/user/cloudray/NORA/scale100/merged/partition_{164,165}.parquet`'\n",
    "# result = spark.sql(sql).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result) # 0 and 1: 3124568\n",
    "# len(result) # 0: 1556604\n",
    "# len(result) # 1: 1567964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kdnode_2_border(kdnode):\n",
    "#     lower = [domain[0] for domain in kdnode[0]]\n",
    "#     upper = [domain[1] for domain in kdnode[0]]\n",
    "#     border = tuple(lower + upper) # non interleave\n",
    "#     return border\n",
    "\n",
    "# def load_partitions_from_file(path):\n",
    "#     '''\n",
    "#     the loaded stretched_kdnodes: [num_dims, l1,l2,...,ln, u1,u2,...,un, size, id, pid, left_child,id, right_child_id]\n",
    "#     '''\n",
    "#     stretched_kdnodes = np.genfromtxt(path, delimiter=',')\n",
    "#     num_dims = int(stretched_kdnodes[0,0])\n",
    "#     kdnodes = []\n",
    "#     for i in range(len(stretched_kdnodes)):\n",
    "#         domains = [ [stretched_kdnodes[i,k+1],stretched_kdnodes[i,1+num_dims+k]] for k in range(num_dims) ]\n",
    "#         row = [domains]\n",
    "#         row.append(stretched_kdnodes[i,2*num_dims+1])\n",
    "#         # to be compatible with qd-tree's partition, that do not have the last 4 attributes\n",
    "#         if len(stretched_kdnodes[i]) > 2*num_dims+2:\n",
    "#             row.append(stretched_kdnodes[i,-4])\n",
    "#             row.append(stretched_kdnodes[i,-3])\n",
    "#             row.append(stretched_kdnodes[i,-2])\n",
    "#             row.append(stretched_kdnodes[i,-1])\n",
    "#         kdnodes.append(row)\n",
    "#     return kdnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitions = load_partitions_from_file(nora_partition)\n",
    "    \n",
    "# p = index.Property()\n",
    "# p.leaf_capacity = 32\n",
    "# p.index_capacity = 32\n",
    "# p.NearMinimumOverlaoFactor = 16\n",
    "# p.fill_factor = 0.8\n",
    "# p.overwrite = True\n",
    "# pidx = index.Index(properties = p)\n",
    "    \n",
    "# partition_index = index.Index(properties = p)\n",
    "# for i in range(len(partitions)):\n",
    "#     partition_index.insert(i, kdnode_2_border(partitions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORA\n",
    "# batch_query(testing_set, used_dims, column_name_dict, hdfs_path_nora, partition_index)\n",
    "\n",
    "# SELECT *\n",
    "# total query response time:  861.4990439414978\n",
    "# average query response time:  17.229980878829956\n",
    "\n",
    "# SELECT COUNT(*) # the advantage is more obvious when io of query result do not dominate the query time\n",
    "# total query response time:  24.595819234848022\n",
    "# average query response time:  0.4919163846969605\n",
    "\n",
    "# SELECT variance(_c0)\n",
    "# total query response time:  32.315288066864014\n",
    "# average query response time:  0.6463057613372802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qd-Tree\n",
    "# batch_query(testing_set, used_dims, column_name_dict, hdfs_path_qdtree)\n",
    "\n",
    "# SELECT *\n",
    "# total query response time:  1169.1192693710327\n",
    "# average query response time:  23.382385387420655\n",
    "\n",
    "# SELECT COUNT(*)\n",
    "# total query response time:  85.07339429855347\n",
    "# average query response time:  1.7014678859710692\n",
    "\n",
    "# SELECT variance(_c0)\n",
    "# total query response time:  102.03884530067444\n",
    "# average query response time:  2.040776906013489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish query 0\n",
      "finish query 1\n",
      "finish query 2\n",
      "finish query 3\n",
      "finish query 4\n",
      "finish query 5\n",
      "finish query 6\n",
      "finish query 7\n",
      "finish query 8\n",
      "finish query 9\n",
      "finish query 10\n",
      "finish query 11\n",
      "finish query 12\n",
      "finish query 13\n",
      "finish query 14\n",
      "finish query 15\n",
      "finish query 16\n",
      "finish query 17\n",
      "finish query 18\n",
      "finish query 19\n",
      "finish query 20\n",
      "finish query 21\n",
      "finish query 22\n",
      "finish query 23\n",
      "finish query 24\n",
      "finish query 25\n",
      "finish query 26\n",
      "finish query 27\n",
      "finish query 28\n",
      "finish query 29\n",
      "finish query 30\n",
      "finish query 31\n",
      "finish query 32\n",
      "finish query 33\n",
      "finish query 34\n",
      "finish query 35\n",
      "finish query 36\n",
      "finish query 37\n",
      "finish query 38\n",
      "finish query 39\n",
      "finish query 40\n",
      "finish query 41\n",
      "finish query 42\n",
      "finish query 43\n",
      "finish query 44\n",
      "finish query 45\n",
      "finish query 46\n",
      "finish query 47\n",
      "finish query 48\n",
      "finish query 49\n",
      "total query response time:  97.01754927635193\n",
      "average query response time:  1.9403509855270387\n"
     ]
    }
   ],
   "source": [
    "# NORA\n",
    "batch_query(testing_set, used_dims, column_name_dict, hdfs_path_nora)\n",
    "\n",
    "# total query response time:  863.7078773975372\n",
    "# average query response time:  17.274157547950743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish query 0\n",
      "finish query 1\n",
      "finish query 2\n",
      "finish query 3\n",
      "finish query 4\n",
      "finish query 5\n",
      "finish query 6\n",
      "finish query 7\n",
      "finish query 8\n",
      "finish query 9\n",
      "finish query 10\n",
      "finish query 11\n",
      "finish query 12\n",
      "finish query 13\n",
      "finish query 14\n",
      "finish query 15\n",
      "finish query 16\n",
      "finish query 17\n",
      "finish query 18\n",
      "finish query 19\n",
      "finish query 20\n",
      "finish query 21\n",
      "finish query 22\n",
      "finish query 23\n",
      "finish query 24\n",
      "finish query 25\n",
      "finish query 26\n",
      "finish query 27\n",
      "finish query 28\n",
      "finish query 29\n",
      "finish query 30\n",
      "finish query 31\n",
      "finish query 32\n",
      "finish query 33\n",
      "finish query 34\n",
      "finish query 35\n",
      "finish query 36\n",
      "finish query 37\n",
      "finish query 38\n",
      "finish query 39\n",
      "finish query 40\n",
      "finish query 41\n",
      "finish query 42\n",
      "finish query 43\n",
      "finish query 44\n",
      "finish query 45\n",
      "finish query 46\n",
      "finish query 47\n",
      "finish query 48\n",
      "finish query 49\n",
      "total query response time:  115.19208145141602\n",
      "average query response time:  2.3038416290283203\n"
     ]
    }
   ],
   "source": [
    "# Qd-Tree\n",
    "batch_query(testing_set, used_dims, column_name_dict, hdfs_path_qdtree)\n",
    "\n",
    "# total query response time:  1192.0776464939117\n",
    "# average query response time:  23.841552929878233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish query 0\n",
      "finish query 1\n",
      "finish query 2\n",
      "finish query 3\n",
      "finish query 4\n",
      "finish query 5\n",
      "finish query 6\n",
      "finish query 7\n",
      "finish query 8\n",
      "finish query 9\n",
      "finish query 10\n",
      "finish query 11\n",
      "finish query 12\n",
      "finish query 13\n",
      "finish query 14\n",
      "finish query 15\n",
      "finish query 16\n",
      "finish query 17\n",
      "finish query 18\n",
      "finish query 19\n",
      "finish query 20\n",
      "finish query 21\n",
      "finish query 22\n",
      "finish query 23\n",
      "finish query 24\n",
      "finish query 25\n",
      "finish query 26\n",
      "finish query 27\n",
      "finish query 28\n",
      "finish query 29\n",
      "finish query 30\n",
      "finish query 31\n",
      "finish query 32\n",
      "finish query 33\n",
      "finish query 34\n",
      "finish query 35\n",
      "finish query 36\n",
      "finish query 37\n",
      "finish query 38\n",
      "finish query 39\n",
      "finish query 40\n",
      "finish query 41\n",
      "finish query 42\n",
      "finish query 43\n",
      "finish query 44\n",
      "finish query 45\n",
      "finish query 46\n",
      "finish query 47\n",
      "finish query 48\n",
      "finish query 49\n",
      "total query response time:  116.32540726661682\n",
      "average query response time:  2.3265081453323364\n"
     ]
    }
   ],
   "source": [
    "# KDTree\n",
    "batch_query(testing_set, used_dims, column_name_dict, hdfs_path_kdtree)\n",
    "\n",
    "# total query response time:  1192.0776464939117\n",
    "# average query response time:  23.841552929878233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
