{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import rtree\n",
    "from rtree import index\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import threading\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([(\"spark.executor.memory\", \"24g\"),(\"spark.driver.memory\",\"24g\"),\n",
    "                           (\"spark.memory.offHeap.enabled\",True),(\"spark.memory.offHeap.size\",\"16g\"),\n",
    "                          (\"spark.driver.maxResultSize\", \"16g\")])\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kdnode_2_border(kdnode):\n",
    "    lower = [domain[0] for domain in kdnode[0]]\n",
    "    upper = [domain[1] for domain in kdnode[0]]\n",
    "    border = tuple(lower + upper) # non interleave\n",
    "    return border\n",
    "\n",
    "def load_partitions_from_file(path):\n",
    "    stretched_kdnodes = genfromtxt(path, delimiter=',')\n",
    "    num_dims = int(stretched_kdnodes[0,0])\n",
    "    kdnodes = []\n",
    "    for i in range(len(stretched_kdnodes)):\n",
    "        domains = [ [stretched_kdnodes[i,k+1],stretched_kdnodes[i,1+num_dims+k]] for k in range(num_dims) ]\n",
    "        row = [domains]\n",
    "        row.append(stretched_kdnodes[i,2*num_dims+1])\n",
    "        # to be compatible with qd-tree's partition, that do not have the last 4 attributes\n",
    "        if len(stretched_kdnodes[i]) > 2*num_dims+2:\n",
    "            row.append(stretched_kdnodes[i,-4])\n",
    "            row.append(stretched_kdnodes[i,-3])\n",
    "            row.append(stretched_kdnodes[i,-2])\n",
    "            row.append(stretched_kdnodes[i,-1])\n",
    "        kdnodes.append(row)\n",
    "    return kdnodes\n",
    "\n",
    "def process_chunk_row(row, used_dims, pidx, pid_data_dict, count, k):\n",
    "    if count[0] % 100000 == 0:\n",
    "        print('proces',k,'has routed',count[0],'rows')\n",
    "    count[0] += 1\n",
    "    row_numpy = row.to_numpy()\n",
    "    row_used_dims_list = row_numpy[used_dims].tolist()\n",
    "    row_border = tuple(row_used_dims_list+row_used_dims_list)\n",
    "    try:\n",
    "        pid = list(pidx.intersection(row_border))[0]\n",
    "    except:\n",
    "        print(row_border)\n",
    "    if pid in pid_data_dict:\n",
    "        pid_data_dict[pid]+=[row_numpy.tolist()]\n",
    "        #print('update dict..')\n",
    "    else:\n",
    "        pid_data_dict[pid]=[row_numpy.tolist()]\n",
    "        #print('initialize dict..')\n",
    "\n",
    "@ray.remote\n",
    "def process_chunk(chunk, used_dims, partition_path, k):\n",
    "    print(\"enter data routing process\", k, '..')    \n",
    "    pid_data_dict = {}\n",
    "    partitions = load_partitions_from_file(partition_path)\n",
    "    p = index.Property()\n",
    "    p.leaf_capacity = 32\n",
    "    p.index_capacity = 32\n",
    "    p.NearMinimumOverlaoFactor = 16\n",
    "    p.fill_factor = 0.8\n",
    "    p.overwrite = True\n",
    "    pidx = index.Index(properties = p)\n",
    "    for i in range(len(partitions)):\n",
    "        pidx.insert(i, kdnode_2_border(partitions[i]))\n",
    "    count = [0]\n",
    "    chunk.apply(lambda row: process_chunk_row(row, used_dims, pidx, pid_data_dict, count, k), axis=1)\n",
    "    dict_id = ray.put(pid_data_dict)\n",
    "    print(\"exit data routing process\", k, \".\")\n",
    "    return dict_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(base_dict, new_dict):\n",
    "    for key, val in new_dict.items():\n",
    "        if key in base_dict:\n",
    "            base_dict[key] += val\n",
    "        else:\n",
    "            base_dict[key] = val\n",
    "    new_dict.clear()\n",
    "\n",
    "def dump_dict_2_hdfs_simple(merged_dict, pq_writers, column_names, hdfs_path, fs):\n",
    "    #print('= = = start dumping in main thread = = =')\n",
    "    for pid, val in merged_dict.items():\n",
    "        #print(\"writing to pid:\",pid)\n",
    "        path = hdfs_path + 'partition_' + str(pid)+'.parquet'\n",
    "        pdf = pd.DataFrame(val, columns=column_names)\n",
    "        adf = pa.Table.from_pandas(pdf)\n",
    "        if pid in pq_writers:\n",
    "            pq_writers[pid].write_table(table=adf)\n",
    "        else:\n",
    "            writer = pq.ParquetWriter(path, adf.schema, fs)\n",
    "            pq_writers[pid] = writer\n",
    "            writer.write_table(table=adf)\n",
    "    #print('= = = exit dumping = = =')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_parallel(table_path, partition_path, chunk_size, used_dims, hdfs_path, num_dims, num_process, hdfs_private_ip):\n",
    "    \n",
    "    begin_time = time.time()\n",
    "    \n",
    "    ray.init(num_cpus=num_process)\n",
    "    \n",
    "    # column names for pandas dataframe\n",
    "    cols = [i for i in range(num_dims)]\n",
    "    col_names = ['_c'+str(i) for i in range(num_dims)]\n",
    "    \n",
    "    # pyarrow parquent append\n",
    "    pq_writers = {}\n",
    "    fs = pa.fs.HadoopFileSystem(hdfs_private_ip, port=9000, user='hdfs', replication=1)\n",
    "    \n",
    "    # chunks\n",
    "    chunk_count = 0\n",
    "    \n",
    "    # collect object refs\n",
    "    result_ids = []\n",
    "    last_batch_ids = [] \n",
    "    first_loop = True\n",
    "    \n",
    "    for chunk in pd.read_table(table_path, delimiter='|', usecols=cols, names=col_names, chunksize=chunk_size):\n",
    "        print('reading chunk: ', chunk_count)\n",
    "        \n",
    "        chunk_id = ray.put(chunk)\n",
    "        result_id = process_chunk.remote(chunk_id, used_dims, partition_path, chunk_count)\n",
    "        \n",
    "        del chunk_id\n",
    "        result_ids.append(result_id)\n",
    "        del result_id\n",
    "        \n",
    "        # after all process allocated a chunk, process and dump the data\n",
    "        if chunk_count % num_process == num_process - 1:\n",
    "            \n",
    "            if first_loop:\n",
    "                first_loop = False\n",
    "                last_batch_ids = result_ids.copy()\n",
    "                result_ids.clear()\n",
    "                chunk_count += 1\n",
    "                continue\n",
    "            else:\n",
    "                print(\"= = = Process Dump For Chunk\", chunk_count-2*num_process+1, \"to\", chunk_count-num_process, \"= = =\")\n",
    "                base_dict = {}\n",
    "                while len(last_batch_ids):\n",
    "                    done_id, last_batch_ids = ray.wait(last_batch_ids)\n",
    "                    dict_id = ray.get(done_id[0])\n",
    "                    result_dict = ray.get(dict_id)\n",
    "                    merge_dict(base_dict, result_dict)\n",
    "                dump_dict_2_hdfs_simple(base_dict, pq_writers, col_names, hdfs_path, fs)\n",
    "                base_dict.clear()\n",
    "                print(\"= = = Finish Dump For Chunk\", chunk_count-2*num_process+1, \"to\", chunk_count-num_process, \"= = =\")\n",
    "                last_batch_ids = result_ids.copy()\n",
    "                result_ids.clear()\n",
    "                \n",
    "            current_time = time.time()\n",
    "            time_elapsed = current_time - begin_time\n",
    "            print(\"= = = TOTAL PROCESSED SO FAR:\", (chunk_count-num_process+1) * chunk_size,\"ROWS. TIME SPENT:\", time_elapsed, \"SECONDS = = =\")\n",
    "                \n",
    "        chunk_count += 1\n",
    "        \n",
    "    # process the last few batches\n",
    "    print(\"= = = Process Dump For Last Few Chunks = = =\")\n",
    "    base_dict = {}\n",
    "    while len(last_batch_ids):\n",
    "        done_id, last_batch_ids = ray.wait(last_batch_ids)\n",
    "        dict_id = ray.get(done_id[0])\n",
    "        result_dict = ray.get(dict_id)\n",
    "        merge_dict(base_dict, result_dict)\n",
    "    dump_dict_2_hdfs_simple(base_dict, pq_writers, col_names, hdfs_path, fs)\n",
    "    base_dict.clear()\n",
    "    last_batch_ids.clear()\n",
    "\n",
    "    base_dict = {}\n",
    "    while len(result_ids):\n",
    "        done_id, result_ids = ray.wait(result_ids)\n",
    "        dict_id = ray.get(done_id[0])\n",
    "        result_dict = ray.get(dict_id)\n",
    "        merge_dict(base_dict, result_dict)\n",
    "    result_ids.clear() # clear up the references\n",
    "    dump_dict_2_hdfs_simple(base_dict, pq_writers, col_names, hdfs_path, fs) \n",
    "    base_dict.clear()\n",
    "    result_ids.clear()\n",
    "\n",
    "    for pid, writer in pq_writers.items():\n",
    "        writer.close()\n",
    "    \n",
    "    ray.shutdown()\n",
    "    \n",
    "    finish_time = time.time()\n",
    "    print('= = = = = TOTAL DATA ROUTING AND PERISITING TIME:', finish_time - begin_time, \"= = = = =\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = Configuration (UBDA Cloud Centos) = = =\n",
    "scale_factor = 100\n",
    "\n",
    "table_base_path = '/media/datadrive1/TPCH/dbgen/'\n",
    "table_path = table_base_path + 'lineitem_' + str(scale_factor) + '.tbl'\n",
    "\n",
    "num_process = 6\n",
    "chunk_size = 2000000 \n",
    "# 6M rows = about 1GB raw data\n",
    "\n",
    "num_dims = 16\n",
    "used_dims = [1,2]\n",
    "\n",
    "# base path of HDFS\n",
    "hdfs_private_ip = '192.168.6.62'\n",
    "hdfs_base_path = 'hdfs://192.168.6.62:9000/user/cloudray/'\n",
    "\n",
    "nora_hdfs = hdfs_base_path + 'NORA/scale' + str(scale_factor) + '/'\n",
    "qdtree_hdfs = hdfs_base_path + 'QdTree/scale' + str(scale_factor) + '/'\n",
    "kdtree_hdfs = hdfs_base_path + 'KDTree/scale' + str(scale_factor) + '/'\n",
    "\n",
    "# base path of Partition\n",
    "partition_base_path = '/home/centos/PartitionLayout/'\n",
    "\n",
    "nora_partition = partition_base_path + 'nora_partitions_' + str(scale_factor)\n",
    "qdtree_partition = partition_base_path + 'qdtree_partitions_' + str(scale_factor)\n",
    "kdtree_partition = partition_base_path + 'kdtree_partitions_' + str(scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-31 14:23:25,483\tINFO services.py:1164 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading chunk:  0\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m enter data routing process 0 ..\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 0 rows\n",
      "reading chunk:  1\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m enter data routing process 1 ..\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 0 rows\n",
      "reading chunk:  2\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m enter data routing process 2 ..\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 200000 rows\n",
      "reading chunk:  3\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m enter data routing process 3 ..\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 500000 rows\n",
      "reading chunk:  4\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m enter data routing process 4 ..\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 200000 rows\n",
      "reading chunk:  5\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m enter data routing process 5 ..\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 200000 rows\n",
      "reading chunk:  6\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 700000 rowsreading chunk:  7\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 700000 rowsreading chunk:  8\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 700000 rows\n",
      "reading chunk:  9\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 900000 rows\n",
      "reading chunk:  10\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 1700000 rows\n",
      "reading chunk:  11\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 1400000 rows\n",
      "= = = Process Dump For Chunk 0 to 5 = = =\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 0 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 1 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m exit data routing process 0 .\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 2 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m enter data routing process 6 ..\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 3 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m exit data routing process 1 .\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m enter data routing process 7 ..\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 4 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m exit data routing process 2 .\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m enter data routing process 8 ..\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m exit data routing process 3 .\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 5 has routed 1900000 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m enter data routing process 9 ..\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m exit data routing process 4 .\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m enter data routing process 10 ..\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m exit data routing process 5 .\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m enter data routing process 11 ..\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 6 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 7 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m exit data routing process 6 .\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 8 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 9 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m exit data routing process 7 .\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 10 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m exit data routing process 8 .\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 1700000 rows= = = Finish Dump For Chunk\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m exit data routing process 9 .\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 1800000 rows\n",
      " 0 to 5 = = =\n",
      "= = = TOTAL PROCESSED SO FAR: 12000000 ROWS. TIME SPENT: 214.67814707756042 SECONDS = = =\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m exit data routing process 10 .\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 11 has routed 1900000 rows\n",
      "reading chunk:  12\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m enter data routing process 12 ..\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 0 rows\n",
      "reading chunk:  13\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m enter data routing process 13 ..\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m exit data routing process 11 .\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 200000 rows\n",
      "reading chunk:  14\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m enter data routing process 14 ..\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 400000 rows\n",
      "reading chunk:  15\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m enter data routing process 15 ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 400000 rows\n",
      "reading chunk:  16\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m enter data routing process 16 ..\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 400000 rows\n",
      "reading chunk:  17\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 800000 rows\n",
      "= = = Process Dump For Chunk 6 to 11 = = =\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m enter data routing process 17 ..\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 1000000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m proces 12 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 1100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 13 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 1200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 1300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31236)\u001b[0m exit data routing process 12 .\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 1400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 14 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m exit data routing process 13 .\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 1500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 15 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 1600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 1700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m exit data routing process 14 .\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 16 has routed 1900000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 1800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m exit data routing process 15 .\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 17 has routed 1900000 rows\n",
      "= = = Finish Dump For Chunk 6 to 11 = = =\n",
      "= = = TOTAL PROCESSED SO FAR: 24000000 ROWS. TIME SPENT: 336.1965925693512 SECONDS = = =\n",
      "reading chunk:  18\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m exit data routing process 16 .\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m enter data routing process 18 ..\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 18 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m exit data routing process 17 .\n",
      "reading chunk:  19\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 18 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m enter data routing process 19 ..\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 19 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 18 has routed 200000 rows\n",
      "reading chunk:  20\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 18 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 19 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m enter data routing process 20 ..\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 20 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 19 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 18 has routed 400000 rows\n",
      "reading chunk:  21\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 20 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 19 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 18 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m enter data routing process 21 ..\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 21 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 20 has routed 200000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 19 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 18 has routed 600000 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading chunk:  22\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 21 has routed 100000 rows\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 20 has routed 300000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m enter data routing process 22 ..\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 18 has routed 700000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 19 has routed 500000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 22 has routed 0 rows\n",
      "\u001b[2m\u001b[36m(pid=31238)\u001b[0m proces 21 has routed 200000 rows\n",
      "reading chunk:  23\n",
      "\u001b[2m\u001b[36m(pid=31239)\u001b[0m proces 20 has routed 400000 rows\n",
      "\u001b[2m\u001b[36m(pid=31237)\u001b[0m proces 18 has routed 800000 rows\n",
      "\u001b[2m\u001b[36m(pid=31234)\u001b[0m proces 19 has routed 600000 rows\n",
      "\u001b[2m\u001b[36m(pid=31235)\u001b[0m proces 22 has routed 100000 rows\n",
      "= = = Process Dump For Chunk 12 to 17 = = =\n"
     ]
    }
   ],
   "source": [
    "# = = = Execution = = =\n",
    "if __name__ == '__main__':\n",
    "    #batch_data_parallel(table_path, nora_partition, chunk_size, used_dims, nora_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "    #print('finish nora data routing..')\n",
    "    #batch_data_parallel(table_path, qdtree_partition, chunk_size, used_dims, qdtree_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "    #print('finish qdtree data routing..')\n",
    "    #batch_data_parallel(table_path, kdtree_partition, chunk_size, used_dims, kdtree_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "    #print('finish kdtree data routing..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0007ad7fa95>:1: DeprecationWarning: pyarrow.hdfs.connect is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
      "  fs = pa.hdfs.connect()\n"
     ]
    }
   ],
   "source": [
    "fs = pa.hdfs.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-ea41433f6b45>:5: DeprecationWarning: pyarrow.hdfs.connect is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
      "  fs = pa.hdfs.connect()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet merge time: 7681.157381057739\n"
     ]
    }
   ],
   "source": [
    "# read every parquet and dump it, see if it has a difference in query response time\n",
    "# use pyarrow !!!\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(421):\n",
    "    pid = i\n",
    "    read_path =  nora_hdfs + 'partition_' + str(pid)+'.parquet'\n",
    "    save_path = nora_hdfs + 'reorganized/partition_' + str(pid)+'.parquet'\n",
    "    df = sqlContext.read.parquet(read_path)\n",
    "    pdf = df.toPandas()\n",
    "    adf = pa.Table.from_pandas(pdf)\n",
    "    fw = fs.open(save_path, 'wb')\n",
    "    pq.write_table(adf, fw)\n",
    "    fw.close()\n",
    "end_time = time.time()\n",
    "print('parquet merge time:',end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done read parquet from path\n",
      "start writing..\n"
     ]
    }
   ],
   "source": [
    "# try partition 9\n",
    "pid = 9\n",
    "read_path = qdtree_hdfs + 'partition_' + str(pid)+'.parquet'\n",
    "save_path = qdtree_hdfs + 'reorganized/partition_' + str(pid)+'.parquet'\n",
    "adf = pq.read_table(read_path)\n",
    "print('done read parquet from path')\n",
    "fw = fs.open(save_path, 'wb')\n",
    "print(\"start writing..\")\n",
    "pq.write_table(adf, fw)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 1 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 2 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 3 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 4 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 5 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 6 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 7 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 8 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 9 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 10 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 11 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 12 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 13 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 14 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 15 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 16 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 17 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 18 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 19 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 20 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 21 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 22 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 23 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 24 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 25 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 26 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 27 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 28 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 29 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 30 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 31 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 32 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 33 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 34 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 35 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 36 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 37 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 38 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 39 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 40 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 41 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 42 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 43 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 44 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 45 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 46 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 47 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 48 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 49 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 50 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 51 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 52 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 53 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 54 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 55 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 56 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 57 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 58 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 59 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 60 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 61 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 62 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 63 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 64 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 65 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 66 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "processing 67 ..\n",
      "done read parquet from path\n",
      "start writing..\n",
      "parquet merge time: 1238.414781332016\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# fs = pa.hdfs.connect()\n",
    "for i in range(68):\n",
    "    print(\"processing\", i, \"..\")\n",
    "    pid = i\n",
    "    read_path = qdtree_hdfs + 'partition_' + str(pid)+'.parquet'\n",
    "    save_path = qdtree_hdfs + 'reorganized/partition_' + str(pid)+'.parquet'\n",
    "    adf = pq.read_table(read_path)\n",
    "    print('done read parquet from path')\n",
    "    fw = fs.open(save_path, 'wb')\n",
    "    print(\"start writing..\")\n",
    "    pq.write_table(adf, fw)\n",
    "    fw.close()\n",
    "end_time = time.time()\n",
    "print('parquet merge time:',end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet merge time: 7264.89306974411\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# fs = pa.hdfs.connect()\n",
    "for i in range(512):\n",
    "    print(\"processing\", i, \"..\")\n",
    "    pid = i\n",
    "    read_path = kdtree_hdfs + 'partition_' + str(pid)+'.parquet'\n",
    "    save_path = kdtree_hdfs + 'reorganized/partition_' + str(pid)+'.parquet'\n",
    "    adf = pq.read_table(read_path)\n",
    "    print('done read parquet from path')\n",
    "    fw = fs.open(save_path, 'wb')\n",
    "    print(\"start writing..\")\n",
    "    pq.write_table(adf, fw)\n",
    "    fw.close()\n",
    "end_time = time.time()\n",
    "print('parquet merge time:',end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
